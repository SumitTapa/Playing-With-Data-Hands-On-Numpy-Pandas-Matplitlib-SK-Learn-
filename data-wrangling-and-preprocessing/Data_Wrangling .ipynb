{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Wrangling: Cleaning, Structuring & Transforming Raw Data\n",
        "\n",
        "Data wrangling (also called data munging or data preparation) is the process of cleaning, structuring, and enriching raw data into a clean, organized format suitable for analysis, reporting, or machine learning. It involves handling missing values, standardizing formats, and merging disparate sources to enable accurate, reliable, and informed decision-making.\n",
        "\n",
        "## Key Aspects of Data Wrangling:\n",
        "\n",
        "*   **Purpose**: To turn, for example, unorganized spreadsheets or disparate databases into a unified, actionable dataset.\n",
        "*   **Core Tasks**: Cleaning (removing duplicates, handling missing values, fixing errors), structuring (reformatting, renaming, changing data types), and enriching (adding new context).\n",
        "*   **The 6-Step Process**: Typically involves **discovery**, **structuring**, **cleaning**, **enriching**, **validating**, and **publishing**.\n",
        "*   **Alternative Names**: Known as data munging, data cleaning, or data preparation.\n",
        "*   **Importance**: It ensures high-quality data, prevents faulty analysis, and saves time by organizing data before it enters a data warehouse or AI model.\n",
        "\n",
        "Data wrangling is often considered an iterative, often manual, process, although many modern tools automate these tasks to handle large datasets.\n",
        "\n",
        "## The 6-Step Data Wrangling Process\n",
        "\n",
        "Based on the provided sources, the data wrangling process typically involves these steps:\n",
        "\n",
        "### 1. Discovery\n",
        "This initial stage focuses on familiarizing yourself with the data. You assess its quality, sources (databases, APIs, CSVs), formats, and potential issues like missing values, inconsistencies, errors, or outliers. The findings are often documented in a data quality or profiling report.\n",
        "\n",
        "### 2. Structuring (or Transformation)\n",
        "Raw data is often unusable in its raw state. This step focuses on organizing the data into a unified format suitable for analysis. Common tasks include:\n",
        "*   **Aggregation**: Combining rows using summary statistics and grouping data based on certain variables.\n",
        "*   **Joining/Merging**: Combining data from multiple tables or disparate sources.\n",
        "*   **Data type conversion**: Changing the data type of a variable (e.g., string to date) to aid in calculations.\n",
        "*   **Pivoting**: Shifting data between rows and columns.\n",
        "\n",
        "### 3. Cleaning\n",
        "This step involves handling missing values (by filling or deleting them), removing duplicates, correcting errors or inconsistencies, and smoothing \"noisy\" data (reducing the impact of random variations). The goal is to ensure as few errors as possible that could influence the final analysis. It's important to avoid unnecessary data loss or overcleaning.\n",
        "\n",
        "### 4. Enriching (or Augmenting)\n",
        "Data enrichment involves adding new information to existing datasets to enhance their value. You assess what additional information is necessary (e.g., demographic, geographic, behavioral data) and integrate it with the existing dataset, applying the same cleaning steps to the new data.\n",
        "\n",
        "### 5. Validating\n",
        "This step verifies the accuracy, consistency, and quality of the wrangled data. Validation techniques include:\n",
        "*   **Data type validation**: Ensuring correct data types.\n",
        "*   **Range or format checks**: Verifying values fall within acceptable ranges and adhere to certain formats.\n",
        "*   **Consistency checks**: Making sure there is a logical agreement between related variables.\n",
        "*   **Uniqueness checks**: Confirming that certain variables (like IDs) have unique values.\n",
        "*   **Statistical analysis**: Identifying outliers or anomalies using descriptive statistics and visualizations.\n",
        "\n",
        "### 6. Publishing\n",
        "Once the data is validated, it is made available for use. This might involve loading it into a data warehouse, creating data visualizations, exporting it for machine learning algorithms, or sharing it with others in the organization via reports or dashboards.\n",
        "\n",
        "## Why Data Wrangling is Important\n",
        "\n",
        "*   **Ensures High-Quality Data**: It addresses data quality issues like missing values, duplicates, and formatting inconsistencies, which are the foundation for accurate analysis.\n",
        "*   **Prevents Faulty Analysis**: Without proper wrangling, the results of data analysis can be misleading, potentially leading to flawed business decisions.\n",
        "*   **Saves Time and Resources**: Although it can be time-consuming (estimates suggest it can take up to 45-80% of an analyst's time), it organizes data so that it's ready for efficient use in downstream processes like building machine learning models, creating data visualizations, and generating business intelligence reports.\n",
        "*   **Enables AI and Machine Learning**: AI models are only as good as the data on which they are trained. Data wrangling helps ensure the information used to develop and enhance models is accurate, improving interpretability and model performance.\n",
        "\n",
        "## Tools and Technologies\n",
        "\n",
        "Organizations use various tools for data wrangling:\n",
        "*   **Programming Languages**: Python (with libraries like Pandas) and R are widely used.\n",
        "*   **Spreadsheets**: Tools like Microsoft Excel and Google Sheets are used for basic cleaning and manipulation of smaller datasets.\n",
        "*   **Specialized Tools**: Platforms like Alteryx, Paxata, and Informatica provide visual interfaces to streamline and automate data cleansing and transformation.\n",
        "*   **Big Data Platforms**: Tools like Apache Hadoop and Apache Spark are used for wrangling large-scale, complex datasets.\n",
        "*   **Cloud Ecosystems**: Cloud providers like AWS, Google Cloud, and Microsoft Azure include data wrangling solutions.\n",
        "\n",
        "In summary, data wrangling is a foundational, iterative process that transforms raw, messy data into a trusted asset, enabling organizations to make informed, data-driven decisions."
      ],
      "metadata": {
        "id": "MhRNFofN_AnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing to a text file using built-in open() function"
      ],
      "metadata": {
        "id": "_3zjqMuz_k0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "directory_path = '/content/Test/'\n",
        "directory_path1 = '/content/'\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "# Define the file path\n",
        "file_path = os.path.join(directory_path, 'example.txt')\n",
        "file_path1 = os.path.join(directory_path1, 'example.txt')\n",
        "\n",
        "# Open the file in write mode ('w') and write content\n",
        "with open(file_path, 'w') as file:\n",
        "  file.write('This is the content of example.txt\\n')\n",
        "  file.write('It was created in the /content/Test/ directory.')\n",
        "with open(file_path1, 'w') as file:\n",
        "  file.write('This is the content of example.txt\\n')\n",
        "  file.write('It was created in the /content/Test/ directory.')\n",
        "\n",
        "print(f\"File '{file_path}' created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhcOm_ikZ__-",
        "outputId": "46d2f066-7a45-4929-c35b-e9d313d2921f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/Test/example.txt' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "HBSYo_Gz-YLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d5fec7-30c6-4952-c095-eef2777ab374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created successfully\n"
          ]
        }
      ],
      "source": [
        "# Open a file named 'example.txt' in write mode ('w').\n",
        "# If the file doesn't exist, it will be created. If it exists, its content will be truncated.\n",
        "# The 'with' statement ensures the file is properly closed after its block finishes.\n",
        "with open('/content/Test/example.txt','w') as file:\n",
        "  # Write the specified string content to the file.\n",
        "  file.write('This is a sample file for testing.\\nHello')\n",
        "# Print a success message to the console.\n",
        "print('File created successfully')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file '/content/Test/example.txt' in write mode ('w').\n",
        "# This will overwrite the file if it already exists or create it if it doesn't.\n",
        "with open('/content/Test/example.txt','w') as file:\n",
        "  # Write a multi-line string to the file.\n",
        "  file.write('''This is a sample file for testing.\\nHello.\n",
        "Testing with multiple lines.\n",
        "Done''')"
      ],
      "metadata": {
        "id": "K5j4I510A_4u"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_list=['Ram','X','Y','Z','A','B','C']\n",
        "# Open the file '/content/Test/example.txt' in write mode ('w').\n",
        "# This will overwrite any existing content.\n",
        "with open('/content/Test/example.txt','w') as file:\n",
        "  # Iterate through each student in the list.\n",
        "  for student in student_list:\n",
        "    # Write each student's name to the file, followed by a newline character.\n",
        "    # The 'write' method does not accept 'end' as a keyword argument.\n",
        "    file.write(f' The student name is {student}.\\n')\n",
        "print(\"TXT with student records printed successfully\")"
      ],
      "metadata": {
        "id": "Oqy7RlO8Bngv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a73a41-fd45-40df-e2a7-db0f1c6ecc95"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TXT with student records printed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_list=['Ram','X','Y','Z','A','B','C']\n",
        "# Open the file '/content/Test/example.txt' in write mode ('w').\n",
        "# This will overwrite any existing content.\n",
        "with open('/content/Test/example.txt','w') as file:\n",
        "  # Iterate through each student in the list.\n",
        "  for student in student_list:\n",
        "    # Write each student's name to the file, followed by a newline character.\n",
        "    # The 'write' method does not accept 'end' as a keyword argument.\n",
        "    file.write(f'The student name is {student}. Welcome onboard {student}\\n')\n",
        "print(\"TXT with student records printed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg9_S6NfA8Aj",
        "outputId": "b7a4f4de-38ec-4477-f2c7-07c23c6e8198"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TXT with student records printed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading an existing file using inbuilt open() function"
      ],
      "metadata": {
        "id": "qZQCX5ZoC6Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the same file 'example.txt' in read mode ('r').\n",
        "# The 'with' statement ensures the file is properly closed.\n",
        "with open('/content/Test/example.txt', 'r') as file:\n",
        "  # Read the entire content of the file.\n",
        "  content = file.read()\n",
        "  # Print the content that was read from the file.\n",
        "  print(content)\n",
        "\n",
        "print('Console Message: File content read successfully.')"
      ],
      "metadata": {
        "id": "okMU4ODsABJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb3e62b-30b0-42a9-a469-c9deee8923de"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student name is Ram. Welcome onboard Ram\n",
            "The student name is X. Welcome onboard X\n",
            "The student name is Y. Welcome onboard Y\n",
            "The student name is Z. Welcome onboard Z\n",
            "The student name is A. Welcome onboard A\n",
            "The student name is B. Welcome onboard B\n",
            "The student name is C. Welcome onboard C\n",
            "\n",
            "Console Message: File content read successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the same file 'example.txt' in read mode ('r').\n",
        "# The 'with' statement ensures the file is properly closed.\n",
        "try:\n",
        "  with open('/content/Test/example1.txt', 'r') as file:\n",
        "    # Read the entire content of the file.\n",
        "    content = file.read()\n",
        "    # Print the content that was read from the file.\n",
        "    print(content)\n",
        "\n",
        "  print('Console Message: File content read successfully.')\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: File not found.\")"
      ],
      "metadata": {
        "id": "-YmrCVTuClc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d400f27-4a97-43e6-9374-de596a68c36e"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HujNbhtsDq0B"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5cbca5"
      },
      "source": [
        "## Key File Reading Operations\n",
        "\n",
        "When working with files in Python, several methods are available for reading their content:\n",
        "\n",
        "*   **`file.read()`**: Reads the entire content of the file as a single string. If an optional `size` argument is provided, it reads up to `size` bytes.\n",
        "\n",
        "    ```python\n",
        "    with open('example.txt', 'r') as file:\n",
        "        content = file.read()\n",
        "        print(content)\n",
        "    ```\n",
        "\n",
        "*   **`file.readline()`**: Reads a single line from the file, including the newline character at the end. Subsequent calls to `readline()` will read the next line.\n",
        "\n",
        "    ```python\n",
        "    with open('example.txt', 'r') as file:\n",
        "        first_line = file.readline()\n",
        "        second_line = file.readline()\n",
        "        print(f\"First line: {first_line}\")\n",
        "        print(f\"Second line: {second_line}\")\n",
        "    ```\n",
        "\n",
        "*   **`file.readlines()`**: Reads all lines from the file and returns them as a list of strings, where each string represents a line and includes the newline character.\n",
        "\n",
        "    ```python\n",
        "    with open('example.txt', 'r') as file:\n",
        "        all_lines = file.readlines()\n",
        "        for line in all_lines:\n",
        "            print(line.strip()) # .strip() removes leading/trailing whitespace, including newline\n",
        "    ```\n",
        "\n",
        "*   **Iterating over a file object**: This is often the most memory-efficient and Pythonic way to read a file line by line, especially for large files.\n",
        "\n",
        "    ```python\n",
        "    with open('example.txt', 'r') as file:\n",
        "        for line in file:\n",
        "            print(line.strip()) # Process each line individually\n",
        "    ```\n",
        "\n",
        "These methods provide flexible ways to access file content based on your specific needs, from reading the whole file at once to processing it line by line."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('example.txt', 'r') as file:\n",
        "    # Read the first line from the file and remove leading/trailing whitespace.\n",
        "    first_line = file.readline().strip()\n",
        "    # Read the second line from the file.\n",
        "    second_line = file.readline()\n",
        "    # Print the first line.\n",
        "    print(f\"First line: {first_line}\")\n",
        "    # Print the second line.\n",
        "    print(f\"Second line: {second_line}\")"
      ],
      "metadata": {
        "id": "pMKelybAELM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0020b976-0dce-44f7-8795-c7117e688d6e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line: This is the content of example.txt\n",
            "Second line: It was created in the /content/Test/ directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('example.txt', 'r') as file:\n",
        "    all_lines = file.readlines()\n",
        "    for line in all_lines:\n",
        "        print(line.strip()) # .strip() removes leading/trailing whitespace, including newline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e-k7Be7ELgH",
        "outputId": "0ce6cfe2-3c6b-4fd7-abc0-9c831d544374"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the content of example.txt\n",
            "It was created in the /content/Test/ directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e275d4"
      },
      "source": [
        "## What is a TSV File?\n",
        "\n",
        "A **TSV (Tab Separated Values)** file is a simple, plain text format used to store tabular data. It is very similar to a CSV (Comma Separated Values) file, but instead of commas, it uses tab characters (`\\t`) to separate values within each row.\n",
        "\n",
        "### Key Concepts of TSV Files:\n",
        "\n",
        "*   **Delimiter**: The primary characteristic is the use of a **tab character (`\\t`)** as the delimiter to separate columns (fields) within a row.\n",
        "*   **Plain Text Format**: TSV files are human-readable and can be opened with any text editor.\n",
        "*   **Tabular Data**: Each line in a TSV file represents a row in a table, and fields within that row are separated by tabs.\n",
        "*   **First Row (Optional)**: Often, the first line of a TSV file contains header labels that describe the content of each column.\n",
        "*   **No Special Escaping (Usually)**: Unlike CSVs, which often require special handling for commas within data fields (e.g., enclosing them in quotes), tabs are less common within data values, so TSV files generally don't require complex quoting or escaping rules.\n",
        "*   **Data Exchange**: Commonly used for data exchange between different programs and systems, especially where data might naturally contain commas (making CSV problematic) or for simpler data parsing.\n",
        "*   **Lightweight**: Because of their simplicity, they are lightweight and easy to process programmatically."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Define possible values for categorical columns\n",
        "first_names = ['Rahul', 'Priya', 'Amit', 'Neeta', 'Raj', 'Anjali', 'Vikram', 'Pooja', 'Sanjay', 'Deepa',\n",
        "               'Arjun', 'Kavita', 'Manoj', 'Shweta', 'Ravi', 'Nidhi', 'Suresh', 'Meera', 'Ajay', 'Divya',\n",
        "               'Vivek', 'Neha', 'Rakesh', 'Anita', 'Ashok', 'Sunita', 'Pankaj', 'Jyoti', 'Nitin', 'Swati',\n",
        "               'Gaurav', 'Ritu', 'Alok', 'Shilpa', 'Anil', 'Rekha', 'Tarun', 'Geeta', 'Harish', 'Preeti']\n",
        "\n",
        "last_names = ['Sharma', 'Patel', 'Singh', 'Rao', 'Kumar', 'Verma', 'Gupta', 'Joshi', 'Reddy', 'Nair',\n",
        "              'Menon', 'Das', 'Bose', 'Chatterjee', 'Mukherjee', 'Banerjee', 'Yadav', 'Jha', 'Sinha', 'Pandey']\n",
        "\n",
        "departments = ['HR', 'Finance', 'IT', 'Marketing', 'Operations', 'Sales', 'R&D', 'Legal', 'Admin', 'Customer Support']\n",
        "cities = ['Mumbai', 'Delhi', 'Bangalore', 'Hyderabad', 'Chennai', 'Pune', 'Ahmedabad', 'Kolkata', 'Jaipur', 'Lucknow']\n",
        "job_titles = {\n",
        "    'HR': ['HR Associate', 'HR Manager', 'Recruiter', 'HR Business Partner', 'HR Director'],\n",
        "    'Finance': ['Accountant', 'Financial Analyst', 'Finance Manager', 'Auditor', 'CFO'],\n",
        "    'IT': ['Software Engineer', 'Senior Developer', 'IT Manager', 'DevOps Engineer', 'CTO'],\n",
        "    'Marketing': ['Marketing Executive', 'Digital Marketing Specialist', 'Brand Manager', 'Marketing Head', 'CMO'],\n",
        "    'Operations': ['Operations Associate', 'Operations Manager', 'Supply Chain Specialist', 'Logistics Coordinator', 'COO'],\n",
        "    'Sales': ['Sales Executive', 'Account Manager', 'Sales Manager', 'Regional Sales Head', 'VP Sales'],\n",
        "    'R&D': ['Research Scientist', 'Product Developer', 'R&D Manager', 'Innovation Lead', 'Director R&D'],\n",
        "    'Legal': ['Legal Counsel', 'Compliance Officer', 'Contract Specialist', 'Legal Manager', 'General Counsel'],\n",
        "    'Admin': ['Administrative Assistant', 'Office Manager', 'Facilities Coordinator', 'Admin Manager', 'Director Admin'],\n",
        "    'Customer Support': ['Support Associate', 'Customer Service Rep', 'Support Manager', 'Client Success Manager', 'Head of Support']\n",
        "}\n",
        "\n",
        "genders = ['Male', 'Female', 'Other']\n",
        "education_levels = ['High School', 'Associate Degree', \"Bachelor's Degree\", \"Master's Degree\", 'PhD']\n",
        "performance_ratings = ['Excellent', 'Good', 'Average', 'Below Average']\n",
        "employment_types = ['Full-time', 'Part-time', 'Contract', 'Intern']\n",
        "marital_statuses = ['Single', 'Married', 'Divorced', 'Widowed']\n",
        "project_names = ['Project Alpha', 'Project Beta', 'Project Gamma', 'Project Delta', 'Project Epsilon',\n",
        "                 'Project Zeta', 'Project Eta', 'Project Theta', 'Project Iota', 'Project Kappa']\n",
        "\n",
        "# Generate employee data\n",
        "num_employees = 200\n",
        "data = {\n",
        "    'Emp_ID': list(range(1001, 1001 + num_employees)),\n",
        "    'Name': [],\n",
        "    'Email': [],\n",
        "    'Gender': [],\n",
        "    'Age': [],\n",
        "    'Department': [],\n",
        "    'Job_Title': [],\n",
        "    'Salary_INR': [],\n",
        "    'Joining_Date': [],\n",
        "    'Years_of_Service': [],\n",
        "    'City': [],\n",
        "    'State': [],\n",
        "    'Education_Level': [],\n",
        "    'Performance_Rating': [],\n",
        "    'Manager_ID': [],\n",
        "    'Project': [],\n",
        "    'Employment_Type': [],\n",
        "    'Marital_Status': [],\n",
        "    'Number_of_Dependents': [],\n",
        "    'Emergency_Contact': []\n",
        "}\n",
        "\n",
        "# State mapping for cities\n",
        "city_to_state = {\n",
        "    'Mumbai': 'Maharashtra', 'Delhi': 'Delhi', 'Bangalore': 'Karnataka', 'Hyderabad': 'Telangana',\n",
        "    'Chennai': 'Tamil Nadu', 'Pune': 'Maharashtra', 'Ahmedabad': 'Gujarat', 'Kolkata': 'West Bengal',\n",
        "    'Jaipur': 'Rajasthan', 'Lucknow': 'Uttar Pradesh'\n",
        "}\n",
        "\n",
        "# Generate manager IDs (some employees will be managers)\n",
        "manager_ids = random.sample(range(1001, 1001 + num_employees), int(num_employees * 0.15))  # 15% are managers\n",
        "\n",
        "# Generate data for each employee\n",
        "for i in range(num_employees):\n",
        "    # Name\n",
        "    first_name = random.choice(first_names)\n",
        "    last_name = random.choice(last_names)\n",
        "    name = f\"{first_name} {last_name}\"\n",
        "    data['Name'].append(name)\n",
        "\n",
        "    # Email\n",
        "    email = f\"{first_name.lower()}.{last_name.lower()}@company.com\"\n",
        "    data['Email'].append(email)\n",
        "\n",
        "    # Gender\n",
        "    data['Gender'].append(random.choice(genders))\n",
        "\n",
        "    # Age (between 22 and 65)\n",
        "    age = random.randint(22, 65)\n",
        "    data['Age'].append(age)\n",
        "\n",
        "    # Department\n",
        "    dept = random.choice(departments)\n",
        "    data['Department'].append(dept)\n",
        "\n",
        "    # Job Title (based on department)\n",
        "    job_title = random.choice(job_titles[dept])\n",
        "    data['Job_Title'].append(job_title)\n",
        "\n",
        "    # Salary (based on job title seniority and age)\n",
        "    base_salary = 30000\n",
        "    if 'Manager' in job_title or 'Head' in job_title or 'Director' in job_title or 'VP' in job_title or 'CFO' in job_title or 'CTO' in job_title:\n",
        "        base_salary = random.randint(120000, 250000)\n",
        "    elif 'Senior' in job_title or 'Lead' in job_title:\n",
        "        base_salary = random.randint(80000, 120000)\n",
        "    elif 'Junior' in job_title or 'Associate' in job_title:\n",
        "        base_salary = random.randint(35000, 55000)\n",
        "    else:\n",
        "        base_salary = random.randint(45000, 90000)\n",
        "\n",
        "    # Adjust salary based on age (experience)\n",
        "    age_factor = age / 30  # older employees generally earn more\n",
        "    salary = int(base_salary * age_factor)\n",
        "    data['Salary_INR'].append(salary)\n",
        "\n",
        "    # Joining Date (random date between 2010 and 2025)\n",
        "    start_date = datetime(2010, 1, 1)\n",
        "    end_date = datetime(2025, 12, 31)\n",
        "    random_days = random.randint(0, (end_date - start_date).days)\n",
        "    joining_date = start_date + timedelta(days=random_days)\n",
        "    data['Joining_Date'].append(joining_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    # Years of Service\n",
        "    today = datetime.now()\n",
        "    years_of_service = (today - joining_date).days / 365.25\n",
        "    data['Years_of_Service'].append(round(years_of_service, 1))\n",
        "\n",
        "    # City\n",
        "    city = random.choice(cities)\n",
        "    data['City'].append(city)\n",
        "\n",
        "    # State\n",
        "    data['State'].append(city_to_state[city])\n",
        "\n",
        "    # Education Level (based on age and job title)\n",
        "    if age < 25:\n",
        "        edu_weights = [0.1, 0.3, 0.5, 0.1, 0.0]  # mostly Bachelor's\n",
        "    elif age > 45 and ('Director' in job_title or 'Manager' in job_title):\n",
        "        edu_weights = [0.0, 0.1, 0.3, 0.4, 0.2]  # more Master's and PhD\n",
        "    else:\n",
        "        edu_weights = [0.05, 0.15, 0.4, 0.35, 0.05]\n",
        "\n",
        "    data['Education_Level'].append(np.random.choice(education_levels, p=edu_weights))\n",
        "\n",
        "    # Performance Rating\n",
        "    perf_weights = [0.2, 0.5, 0.25, 0.05]  # mostly Good and Average\n",
        "    data['Performance_Rating'].append(np.random.choice(performance_ratings, p=perf_weights))\n",
        "\n",
        "    # Manager ID (assign manager or None)\n",
        "    if i in manager_ids:\n",
        "        data['Manager_ID'].append(None)  # Manager has no manager\n",
        "    else:\n",
        "        # Assign a random manager (ensure not self)\n",
        "        possible_managers = [m for m in manager_ids if m != data['Emp_ID'][i]]\n",
        "        data['Manager_ID'].append(random.choice(possible_managers) if possible_managers else None)\n",
        "\n",
        "    # Project\n",
        "    data['Project'].append(random.choice(project_names))\n",
        "\n",
        "    # Employment Type\n",
        "    emp_weights = [0.8, 0.1, 0.07, 0.03]  # mostly full-time\n",
        "    data['Employment_Type'].append(np.random.choice(employment_types, p=emp_weights))\n",
        "\n",
        "    # Marital Status\n",
        "    if age < 25:\n",
        "        ms_weights = [0.8, 0.15, 0.03, 0.02]\n",
        "    elif age > 40:\n",
        "        ms_weights = [0.1, 0.7, 0.15, 0.05]\n",
        "    else:\n",
        "        ms_weights = [0.4, 0.5, 0.08, 0.02]\n",
        "    data['Marital_Status'].append(np.random.choice(marital_statuses, p=ms_weights))\n",
        "\n",
        "    # Number of Dependents\n",
        "    if data['Marital_Status'][i] == 'Married':\n",
        "        dependents = random.choices([0, 1, 2, 3, 4], weights=[0.2, 0.3, 0.3, 0.15, 0.05])[0]\n",
        "    else:\n",
        "        dependents = random.choices([0, 1, 2], weights=[0.7, 0.2, 0.1])[0]\n",
        "    data['Number_of_Dependents'].append(dependents)\n",
        "\n",
        "    # Emergency Contact (random phone number)\n",
        "    data['Emergency_Contact'].append(f\"+91-{random.randint(7000000000, 9999999999)}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head(10))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nDataset Info:\")\n",
        "print(f\"Total employees: {len(df)}\")\n",
        "print(f\"Departments: {df['Department'].nunique()}\")\n",
        "print(f\"Average salary: ₹{df['Salary_INR'].mean():,.0f}\")\n",
        "print(f\"Salary range: ₹{df['Salary_INR'].min():,} - ₹{df['Salary_INR'].max():,}\")\n",
        "print(f\"Date range: {df['Joining_Date'].min()} to {df['Joining_Date'].max()}\")\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('dummy_employee_data.csv', index=False)\n",
        "print(\"\\nDataset saved to 'dummy_employee_data.csv'\")\n",
        "\n",
        "# Show distribution\n",
        "print(\"\\nDepartment distribution:\")\n",
        "print(df['Department'].value_counts())\n",
        "\n",
        "print(\"\\nEmployment type distribution:\")\n",
        "print(df['Employment_Type'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9xGPO7iETxF",
        "outputId": "46ab2a8e-c008-43f0-dff8-8f7b21664ebc"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Emp_ID             Name  ... Number_of_Dependents Emergency_Contact\n",
            "0    1001      Geeta Reddy  ...                    0    +91-8631775357\n",
            "1    1002       Vikram Das  ...                    0    +91-8259191105\n",
            "2    1003       Preeti Das  ...                    1    +91-8632629719\n",
            "3    1004  Meera Mukherjee  ...                    2    +91-7735034881\n",
            "4    1005       Anil Joshi  ...                    3    +91-7240251661\n",
            "5    1006       Ravi Patel  ...                    1    +91-9761027762\n",
            "6    1007      Swati Kumar  ...                    1    +91-7941975480\n",
            "7    1008     Sanjay Yadav  ...                    0    +91-8652563013\n",
            "8    1009     Ashok Pandey  ...                    1    +91-9752909971\n",
            "9    1010         Neha Rao  ...                    2    +91-7457031004\n",
            "\n",
            "[10 rows x 20 columns]\n",
            "\n",
            "Dataset Info:\n",
            "Total employees: 200\n",
            "Departments: 10\n",
            "Average salary: ₹169,515\n",
            "Salary range: ₹36,007 - ₹514,386\n",
            "Date range: 2010-01-10 to 2025-10-28\n",
            "\n",
            "Dataset saved to 'dummy_employee_data.csv'\n",
            "\n",
            "Department distribution:\n",
            "Department\n",
            "Operations          27\n",
            "Marketing           24\n",
            "Admin               24\n",
            "Sales               22\n",
            "R&D                 20\n",
            "Legal               20\n",
            "IT                  19\n",
            "Finance             16\n",
            "HR                  14\n",
            "Customer Support    14\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Employment type distribution:\n",
            "Employment_Type\n",
            "Full-time    166\n",
            "Part-time     15\n",
            "Contract      13\n",
            "Intern         6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(data)\n",
        "# print(df) explicitly outputs the DataFrame to standard output.\n",
        "# The output formatting might be simpler compared to the rich display of a DataFrame.\n",
        "print(df)\n",
        "# When a DataFrame (or any expression) is the last line in a Colab cell,\n",
        "# it's automatically displayed as the cell's rich output, often with better formatting and interactivity.\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "Dny_JXvrGKrb",
        "outputId": "811a2a29-24ff-4d7a-e10a-3bf02e5668bf"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Emp_ID             Name  ... Number_of_Dependents Emergency_Contact\n",
            "0      1001      Geeta Reddy  ...                    0    +91-8631775357\n",
            "1      1002       Vikram Das  ...                    0    +91-8259191105\n",
            "2      1003       Preeti Das  ...                    1    +91-8632629719\n",
            "3      1004  Meera Mukherjee  ...                    2    +91-7735034881\n",
            "4      1005       Anil Joshi  ...                    3    +91-7240251661\n",
            "..      ...              ...  ...                  ...               ...\n",
            "195    1196        Neeta Jha  ...                    2    +91-7194518221\n",
            "196    1197        Jyoti Das  ...                    0    +91-7581662546\n",
            "197    1198       Pankaj Das  ...                    0    +91-9307507627\n",
            "198    1199     Sunita Yadav  ...                    1    +91-7986399651\n",
            "199    1200  Nidhi Mukherjee  ...                    0    +91-7067198189\n",
            "\n",
            "[200 rows x 20 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Emp_ID             Name  ... Number_of_Dependents Emergency_Contact\n",
              "0      1001      Geeta Reddy  ...                    0    +91-8631775357\n",
              "1      1002       Vikram Das  ...                    0    +91-8259191105\n",
              "2      1003       Preeti Das  ...                    1    +91-8632629719\n",
              "3      1004  Meera Mukherjee  ...                    2    +91-7735034881\n",
              "4      1005       Anil Joshi  ...                    3    +91-7240251661\n",
              "..      ...              ...  ...                  ...               ...\n",
              "195    1196        Neeta Jha  ...                    2    +91-7194518221\n",
              "196    1197        Jyoti Das  ...                    0    +91-7581662546\n",
              "197    1198       Pankaj Das  ...                    0    +91-9307507627\n",
              "198    1199     Sunita Yadav  ...                    1    +91-7986399651\n",
              "199    1200  Nidhi Mukherjee  ...                    0    +91-7067198189\n",
              "\n",
              "[200 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bafc7c2-057b-4681-9350-b41e7dfd99e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emp_ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Email</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Department</th>\n",
              "      <th>Job_Title</th>\n",
              "      <th>Salary_INR</th>\n",
              "      <th>Joining_Date</th>\n",
              "      <th>Years_of_Service</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Education_Level</th>\n",
              "      <th>Performance_Rating</th>\n",
              "      <th>Manager_ID</th>\n",
              "      <th>Project</th>\n",
              "      <th>Employment_Type</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Number_of_Dependents</th>\n",
              "      <th>Emergency_Contact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>Geeta Reddy</td>\n",
              "      <td>geeta.reddy@company.com</td>\n",
              "      <td>Male</td>\n",
              "      <td>32</td>\n",
              "      <td>R&amp;D</td>\n",
              "      <td>R&amp;D Manager</td>\n",
              "      <td>166849</td>\n",
              "      <td>2013-06-27</td>\n",
              "      <td>12.7</td>\n",
              "      <td>Hyderabad</td>\n",
              "      <td>Telangana</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Below Average</td>\n",
              "      <td>1051</td>\n",
              "      <td>Project Zeta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-8631775357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>Vikram Das</td>\n",
              "      <td>vikram.das@company.com</td>\n",
              "      <td>Female</td>\n",
              "      <td>60</td>\n",
              "      <td>Operations</td>\n",
              "      <td>Operations Associate</td>\n",
              "      <td>100108</td>\n",
              "      <td>2022-01-10</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Associate Degree</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>1115</td>\n",
              "      <td>Project Eta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-8259191105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>Preeti Das</td>\n",
              "      <td>preeti.das@company.com</td>\n",
              "      <td>Other</td>\n",
              "      <td>34</td>\n",
              "      <td>Finance</td>\n",
              "      <td>Accountant</td>\n",
              "      <td>100114</td>\n",
              "      <td>2015-02-10</td>\n",
              "      <td>11.0</td>\n",
              "      <td>Chennai</td>\n",
              "      <td>Tamil Nadu</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Average</td>\n",
              "      <td>1007</td>\n",
              "      <td>Project Delta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>1</td>\n",
              "      <td>+91-8632629719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>Meera Mukherjee</td>\n",
              "      <td>meera.mukherjee@company.com</td>\n",
              "      <td>Other</td>\n",
              "      <td>45</td>\n",
              "      <td>IT</td>\n",
              "      <td>IT Manager</td>\n",
              "      <td>249849</td>\n",
              "      <td>2014-09-13</td>\n",
              "      <td>11.5</td>\n",
              "      <td>Chennai</td>\n",
              "      <td>Tamil Nadu</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1198</td>\n",
              "      <td>Project Beta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>2</td>\n",
              "      <td>+91-7735034881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>Anil Joshi</td>\n",
              "      <td>anil.joshi@company.com</td>\n",
              "      <td>Male</td>\n",
              "      <td>51</td>\n",
              "      <td>R&amp;D</td>\n",
              "      <td>R&amp;D Manager</td>\n",
              "      <td>424486</td>\n",
              "      <td>2024-05-09</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Jaipur</td>\n",
              "      <td>Rajasthan</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1036</td>\n",
              "      <td>Project Zeta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>3</td>\n",
              "      <td>+91-7240251661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>1196</td>\n",
              "      <td>Neeta Jha</td>\n",
              "      <td>neeta.jha@company.com</td>\n",
              "      <td>Female</td>\n",
              "      <td>65</td>\n",
              "      <td>Operations</td>\n",
              "      <td>Operations Associate</td>\n",
              "      <td>103943</td>\n",
              "      <td>2015-09-16</td>\n",
              "      <td>10.4</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Maharashtra</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1144</td>\n",
              "      <td>Project Delta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>2</td>\n",
              "      <td>+91-7194518221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>1197</td>\n",
              "      <td>Jyoti Das</td>\n",
              "      <td>jyoti.das@company.com</td>\n",
              "      <td>Other</td>\n",
              "      <td>26</td>\n",
              "      <td>Admin</td>\n",
              "      <td>Administrative Assistant</td>\n",
              "      <td>42919</td>\n",
              "      <td>2020-08-03</td>\n",
              "      <td>5.6</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Maharashtra</td>\n",
              "      <td>Associate Degree</td>\n",
              "      <td>Average</td>\n",
              "      <td>1027</td>\n",
              "      <td>Project Eta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-7581662546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>1198</td>\n",
              "      <td>Pankaj Das</td>\n",
              "      <td>pankaj.das@company.com</td>\n",
              "      <td>Female</td>\n",
              "      <td>50</td>\n",
              "      <td>R&amp;D</td>\n",
              "      <td>Innovation Lead</td>\n",
              "      <td>142098</td>\n",
              "      <td>2025-04-24</td>\n",
              "      <td>0.8</td>\n",
              "      <td>Jaipur</td>\n",
              "      <td>Rajasthan</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Average</td>\n",
              "      <td>1071</td>\n",
              "      <td>Project Zeta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-9307507627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>1199</td>\n",
              "      <td>Sunita Yadav</td>\n",
              "      <td>sunita.yadav@company.com</td>\n",
              "      <td>Male</td>\n",
              "      <td>36</td>\n",
              "      <td>HR</td>\n",
              "      <td>HR Associate</td>\n",
              "      <td>53731</td>\n",
              "      <td>2020-05-20</td>\n",
              "      <td>5.8</td>\n",
              "      <td>Jaipur</td>\n",
              "      <td>Rajasthan</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1152</td>\n",
              "      <td>Project Iota</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>1</td>\n",
              "      <td>+91-7986399651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>1200</td>\n",
              "      <td>Nidhi Mukherjee</td>\n",
              "      <td>nidhi.mukherjee@company.com</td>\n",
              "      <td>Female</td>\n",
              "      <td>31</td>\n",
              "      <td>Operations</td>\n",
              "      <td>Operations Manager</td>\n",
              "      <td>250963</td>\n",
              "      <td>2012-07-13</td>\n",
              "      <td>13.6</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Maharashtra</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1167</td>\n",
              "      <td>Project Eta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-7067198189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bafc7c2-057b-4681-9350-b41e7dfd99e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3bafc7c2-057b-4681-9350-b41e7dfd99e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3bafc7c2-057b-4681-9350-b41e7dfd99e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_35080954-be6a-4ef4-ba64-c2a052fef062\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_35080954-be6a-4ef4-ba64-c2a052fef062 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"Emp_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 1001,\n        \"max\": 1200,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          1096,\n          1016,\n          1031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"Arjun Chatterjee\",\n          \"Nidhi Reddy\",\n          \"Raj Bose\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"arjun.chatterjee@company.com\",\n          \"nidhi.reddy@company.com\",\n          \"raj.bose@company.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Male\",\n          \"Female\",\n          \"Other\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 22,\n        \"max\": 65,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          33,\n          24,\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Department\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Legal\",\n          \"Operations\",\n          \"HR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job_Title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"Admin Manager\",\n          \"Support Associate\",\n          \"Legal Manager\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Salary_INR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 105839,\n        \"min\": 36007,\n        \"max\": 514386,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          151601,\n          104440,\n          130105\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Joining_Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 198,\n        \"samples\": [\n          \"2023-05-16\",\n          \"2019-12-01\",\n          \"2019-03-22\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Years_of_Service\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.602406815185107,\n        \"min\": 0.3,\n        \"max\": 16.1,\n        \"num_unique_values\": 122,\n        \"samples\": [\n          14.8,\n          0.8,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Mumbai\",\n          \"Delhi\",\n          \"Lucknow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"State\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"West Bengal\",\n          \"Delhi\",\n          \"Uttar Pradesh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education_Level\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Associate Degree\",\n          \"PhD\",\n          \"Master's Degree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Performance_Rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Excellent\",\n          \"Good\",\n          \"Below Average\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Manager_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 1007,\n        \"max\": 1198,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1009,\n          1130,\n          1071\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Project Alpha\",\n          \"Project Eta\",\n          \"Project Kappa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Employment_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Contract\",\n          \"Intern\",\n          \"Full-time\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Marital_Status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Divorced\",\n          \"Widowed\",\n          \"Married\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number_of_Dependents\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emergency_Contact\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"+91-9660933507\",\n          \"+91-9031403296\",\n          \"+91-8745377599\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a Tab Separated Values (TSV) file.\n",
        "# 'dummy_tsv_example.tsv' is the output filename.\n",
        "# 'sep='\\t'' specifies that columns should be separated by tabs.\n",
        "# 'index=False' excludes the DataFrame's index from the TSV file.\n",
        "# 'index_label='Row_ID'' sets the header for the index column to 'Row_ID'.\n",
        "df.to_csv('dummy_tsv_example.tsv', sep='\\t', index=False, index_label='Row_ID')"
      ],
      "metadata": {
        "id": "pbjxls2XG0Bl"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the TSV file 'dummy_tsv_example.tsv' into a pandas DataFrame.\n",
        "# The 'sep='\\t'' argument specifies that the file is tab-separated.\n",
        "df_read=pd.read_csv('dummy_tsv_example.tsv',sep='\\t')\n",
        "# Display the first 5 rows of the DataFrame to verify successful loading.\n",
        "df_read.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "LnuOdoaWLlD1",
        "outputId": "0be27a1f-fd72-4995-c7fa-d713e04d3ede"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Emp_ID             Name  ... Number_of_Dependents Emergency_Contact\n",
              "0    1001      Geeta Reddy  ...                    0    +91-8631775357\n",
              "1    1002       Vikram Das  ...                    0    +91-8259191105\n",
              "2    1003       Preeti Das  ...                    1    +91-8632629719\n",
              "3    1004  Meera Mukherjee  ...                    2    +91-7735034881\n",
              "4    1005       Anil Joshi  ...                    3    +91-7240251661\n",
              "\n",
              "[5 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b1b86d7-c9be-42f1-91b6-4d2694c33e59\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emp_ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Email</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Department</th>\n",
              "      <th>Job_Title</th>\n",
              "      <th>Salary_INR</th>\n",
              "      <th>Joining_Date</th>\n",
              "      <th>Years_of_Service</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Education_Level</th>\n",
              "      <th>Performance_Rating</th>\n",
              "      <th>Manager_ID</th>\n",
              "      <th>Project</th>\n",
              "      <th>Employment_Type</th>\n",
              "      <th>Marital_Status</th>\n",
              "      <th>Number_of_Dependents</th>\n",
              "      <th>Emergency_Contact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1001</td>\n",
              "      <td>Geeta Reddy</td>\n",
              "      <td>geeta.reddy@company.com</td>\n",
              "      <td>Male</td>\n",
              "      <td>32</td>\n",
              "      <td>R&amp;D</td>\n",
              "      <td>R&amp;D Manager</td>\n",
              "      <td>166849</td>\n",
              "      <td>2013-06-27</td>\n",
              "      <td>12.7</td>\n",
              "      <td>Hyderabad</td>\n",
              "      <td>Telangana</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Below Average</td>\n",
              "      <td>1051</td>\n",
              "      <td>Project Zeta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-8631775357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002</td>\n",
              "      <td>Vikram Das</td>\n",
              "      <td>vikram.das@company.com</td>\n",
              "      <td>Female</td>\n",
              "      <td>60</td>\n",
              "      <td>Operations</td>\n",
              "      <td>Operations Associate</td>\n",
              "      <td>100108</td>\n",
              "      <td>2022-01-10</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Associate Degree</td>\n",
              "      <td>Excellent</td>\n",
              "      <td>1115</td>\n",
              "      <td>Project Eta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>0</td>\n",
              "      <td>+91-8259191105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1003</td>\n",
              "      <td>Preeti Das</td>\n",
              "      <td>preeti.das@company.com</td>\n",
              "      <td>Other</td>\n",
              "      <td>34</td>\n",
              "      <td>Finance</td>\n",
              "      <td>Accountant</td>\n",
              "      <td>100114</td>\n",
              "      <td>2015-02-10</td>\n",
              "      <td>11.0</td>\n",
              "      <td>Chennai</td>\n",
              "      <td>Tamil Nadu</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Average</td>\n",
              "      <td>1007</td>\n",
              "      <td>Project Delta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>1</td>\n",
              "      <td>+91-8632629719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1004</td>\n",
              "      <td>Meera Mukherjee</td>\n",
              "      <td>meera.mukherjee@company.com</td>\n",
              "      <td>Other</td>\n",
              "      <td>45</td>\n",
              "      <td>IT</td>\n",
              "      <td>IT Manager</td>\n",
              "      <td>249849</td>\n",
              "      <td>2014-09-13</td>\n",
              "      <td>11.5</td>\n",
              "      <td>Chennai</td>\n",
              "      <td>Tamil Nadu</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1198</td>\n",
              "      <td>Project Beta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>2</td>\n",
              "      <td>+91-7735034881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1005</td>\n",
              "      <td>Anil Joshi</td>\n",
              "      <td>anil.joshi@company.com</td>\n",
              "      <td>Male</td>\n",
              "      <td>51</td>\n",
              "      <td>R&amp;D</td>\n",
              "      <td>R&amp;D Manager</td>\n",
              "      <td>424486</td>\n",
              "      <td>2024-05-09</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Jaipur</td>\n",
              "      <td>Rajasthan</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>Good</td>\n",
              "      <td>1036</td>\n",
              "      <td>Project Zeta</td>\n",
              "      <td>Full-time</td>\n",
              "      <td>Married</td>\n",
              "      <td>3</td>\n",
              "      <td>+91-7240251661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b1b86d7-c9be-42f1-91b6-4d2694c33e59')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b1b86d7-c9be-42f1-91b6-4d2694c33e59 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b1b86d7-c9be-42f1-91b6-4d2694c33e59');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_read",
              "summary": "{\n  \"name\": \"df_read\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"Emp_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 1001,\n        \"max\": 1200,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          1096,\n          1016,\n          1031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"Arjun Chatterjee\",\n          \"Nidhi Reddy\",\n          \"Raj Bose\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"arjun.chatterjee@company.com\",\n          \"nidhi.reddy@company.com\",\n          \"raj.bose@company.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Male\",\n          \"Female\",\n          \"Other\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 22,\n        \"max\": 65,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          33,\n          24,\n          44\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Department\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Legal\",\n          \"Operations\",\n          \"HR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job_Title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"Admin Manager\",\n          \"Support Associate\",\n          \"Legal Manager\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Salary_INR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 105839,\n        \"min\": 36007,\n        \"max\": 514386,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          151601,\n          104440,\n          130105\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Joining_Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 198,\n        \"samples\": [\n          \"2023-05-16\",\n          \"2019-12-01\",\n          \"2019-03-22\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Years_of_Service\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.602406815185107,\n        \"min\": 0.3,\n        \"max\": 16.1,\n        \"num_unique_values\": 122,\n        \"samples\": [\n          14.8,\n          0.8,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Mumbai\",\n          \"Delhi\",\n          \"Lucknow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"State\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"West Bengal\",\n          \"Delhi\",\n          \"Uttar Pradesh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education_Level\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Associate Degree\",\n          \"PhD\",\n          \"Master's Degree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Performance_Rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Excellent\",\n          \"Good\",\n          \"Below Average\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Manager_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 1007,\n        \"max\": 1198,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1009,\n          1130,\n          1071\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Project\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Project Alpha\",\n          \"Project Eta\",\n          \"Project Kappa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Employment_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Contract\",\n          \"Intern\",\n          \"Full-time\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Marital_Status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Divorced\",\n          \"Widowed\",\n          \"Married\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number_of_Dependents\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emergency_Contact\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"+91-9660933507\",\n          \"+91-9031403296\",\n          \"+91-8745377599\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll explain the differences between CSV and TSV formats in the context of pandas data wrangling, formatted for a Jupyter text cell.\n",
        "\n",
        "```python\n",
        "# CSV vs TSV in Pandas Data Wrangling\n",
        "\n",
        "CSV (Comma-Separated Values) and TSV (Tab-Separated Values) are both delimited text file formats used to store tabular data. The key difference is the character used to separate values: commas in CSV and tabs in TSV.\n",
        "\n",
        "## Quick Comparison Table\n",
        "\n",
        "| Feature | CSV | TSV |\n",
        "|---------|-----|-----|\n",
        "| Delimiter | Comma (`,`) | Tab (`\\t`) |\n",
        "| File Extension | `.csv` | `.tsv` or `.txt` |\n",
        "| Common Use | General data exchange, Excel export | Bioinformatics, datasets with text containing commas |\n",
        "| Reading in pandas | `pd.read_csv('file.csv')` | `pd.read_csv('file.tsv', sep='\\t')` |\n",
        "| Writing in pandas | `df.to_csv('file.csv')` | `df.to_csv('file.tsv', sep='\\t')` |\n",
        "\n",
        "## Key Considerations for Data Wrangling\n",
        "\n",
        "### 1. Handling Commas in Data\n",
        "CSV files can break when data fields contain commas (e.g., addresses, descriptive text). TSV files avoid this issue because tabs rarely appear in natural text.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Problematic CSV with comma in data\n",
        "# \"Name\",\"Address\",\"City\"\n",
        "# \"John Smith\",\"123 Main St, Apt 4B\",\"New York\"\n",
        "\n",
        "# This would be parsed incorrectly because the address contains a comma\n",
        "\n",
        "# TSV handles this gracefully\n",
        "# \"Name\"\\t\"Address\"\\t\"City\"\n",
        "# \"John Smith\"\\t\"123 Main St, Apt 4B\"\\t\"New York\"\n",
        "```\n",
        "\n",
        "### 2. Reading Files in Pandas\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Reading CSV (default delimiter is comma)\n",
        "df_csv = pd.read_csv('data.csv')\n",
        "\n",
        "# Reading TSV (explicitly specify tab delimiter)\n",
        "df_tsv = pd.read_csv('data.tsv', sep='\\t')\n",
        "\n",
        "# Alternative: use the '\\t' escape sequence\n",
        "df_tsv2 = pd.read_csv('data.tsv', delimiter='\\t')\n",
        "\n",
        "# Reading TSV with .tsv extension (pandas automatically detects?)\n",
        "# Note: pandas doesn't auto-detect based on extension; always specify sep for TSV\n",
        "```\n",
        "\n",
        "### 3. Writing Files in Pandas\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Occupation': ['Data Scientist', 'Software Engineer', 'Product Manager'],\n",
        "    'Description': ['Loves Python, pandas', 'Works on web apps, APIs', 'Manages teams, roadmaps']\n",
        "})\n",
        "\n",
        "# Write to CSV (default)\n",
        "df.to_csv('output.csv', index=False)\n",
        "\n",
        "# Write to TSV (specify tab separator)\n",
        "df.to_csv('output.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write to TSV with .txt extension\n",
        "df.to_csv('output.txt', sep='\\t', index=False)\n",
        "```\n",
        "\n",
        "### 4. Handling Different Delimiters in the Same File\n",
        "\n",
        "Sometimes files use mixed delimiters or inconsistent formatting:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# If you're unsure of the delimiter, let pandas try to detect it\n",
        "df = pd.read_csv('unknown_delimiter.txt', sep=None, engine='python')\n",
        "\n",
        "# For files with inconsistent delimiters, you might need preprocessing\n",
        "with open('messy_data.csv', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Clean lines and standardize delimiter\n",
        "cleaned_lines = [line.replace(';', ',').replace('\\t', ',') for line in lines]\n",
        "\n",
        "# Write cleaned data to temporary file or use StringIO\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(''.join(cleaned_lines)))\n",
        "```\n",
        "\n",
        "### 5. Memory and Performance Considerations\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# For large files, specifying the delimiter explicitly improves performance\n",
        "# (pandas doesn't need to guess)\n",
        "\n",
        "# CSV (explicit is still good practice)\n",
        "df_csv = pd.read_csv('large_file.csv', sep=',')\n",
        "\n",
        "# TSV\n",
        "df_tsv = pd.read_csv('large_file.tsv', sep='\\t')\n",
        "\n",
        "# Use chunks for very large files\n",
        "chunk_iter = pd.read_csv('massive_file.tsv', sep='\\t', chunksize=10000)\n",
        "for chunk in chunk_iter:\n",
        "    # process each chunk\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 6. Practical Example: Converting Between Formats\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Read TSV, write CSV\n",
        "df = pd.read_csv('data.tsv', sep='\\t')\n",
        "df.to_csv('data_converted.csv', index=False)\n",
        "\n",
        "# Read CSV, write TSV\n",
        "df = pd.read_csv('data.csv')\n",
        "df.to_csv('data_converted.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Bulk conversion of multiple files\n",
        "import glob\n",
        "\n",
        "# Convert all CSV files to TSV\n",
        "for csv_file in glob.glob('*.csv'):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    tsv_file = csv_file.replace('.csv', '.tsv')\n",
        "    df.to_csv(tsv_file, sep='\\t', index=False)\n",
        "    print(f\"Converted {csv_file} to {tsv_file}\")\n",
        "```\n",
        "\n",
        "## When to Use Each Format\n",
        "\n",
        "### Use CSV when:\n",
        "- Working with Excel users (Excel opens CSV by default)\n",
        "- Sharing data with systems that expect CSV format\n",
        "- Data doesn't contain commas in text fields\n",
        "- You need maximum compatibility with legacy systems\n",
        "\n",
        "### Use TSV when:\n",
        "- Data contains commas (addresses, descriptions, names with suffixes)\n",
        "- Working with bioinformatics data (common in genomics)\n",
        "- Avoiding delimiter conflicts is critical\n",
        "- Processing text-heavy datasets\n",
        "\n",
        "### Use TSV with caution when:\n",
        "- Data might contain actual tab characters (rare in most datasets)\n",
        "- Sharing with non-technical users who might not know it's tab-delimited\n",
        "\n",
        "## Best Practices for Data Wrangling\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Always specify the delimiter explicitly\n",
        "df = pd.read_csv('data.tsv', sep='\\t')  # Good\n",
        "# df = pd.read_csv('data.tsv')  # Bad - will try comma delimiter\n",
        "\n",
        "# 2. Quote handling for CSV\n",
        "df = pd.read_csv('data.csv', quoting=1)  # QUOTE_ALL = 1, QUOTE_MINIMAL = 0\n",
        "\n",
        "# 3. Handle encoding properly\n",
        "df = pd.read_csv('data.csv', encoding='utf-8')  # or 'latin-1', 'cp1252'\n",
        "\n",
        "# 4. Inspect first few rows to verify correct parsing\n",
        "df = pd.read_csv('data.tsv', sep='\\t', nrows=5)\n",
        "print(df.head())\n",
        "print(df.dtypes)\n",
        "\n",
        "# 5. For files with headers, use header parameter\n",
        "df = pd.read_csv('data.tsv', sep='\\t', header=0)  # First row is header\n",
        "df = pd.read_csv('data.tsv', sep='\\t', header=None)  # No header\n",
        "\n",
        "# 6. Specify column names manually if needed\n",
        "df = pd.read_csv('data.tsv', sep='\\t', names=['col1', 'col2', 'col3'])\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "| Aspect | CSV | TSV |\n",
        "|--------|-----|-----|\n",
        "| **Delimiter** | Comma (`,`) | Tab (`\\t`) |\n",
        "| **Pandas Read** | `pd.read_csv()` | `pd.read_csv(sep='\\t')` |\n",
        "| **Pandas Write** | `df.to_csv()` | `df.to_csv(sep='\\t')` |\n",
        "| **Pros** | Universal compatibility | Handles commas in data well |\n",
        "| **Cons** | Breaks with commas in data | Less common, may confuse users |\n",
        "| **Best For** | Simple data, Excel exchange | Text data, bioinformatics |\n",
        "\n",
        "The choice between CSV and TSV often comes down to your data content. If your data contains commas, use TSV. For maximum compatibility with other tools and users, CSV remains the standard choice.\n",
        "```\n"
      ],
      "metadata": {
        "id": "JPGif8XsOVRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0BnsPfzaOkYl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w33UgPG1NaTR"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edcc8c58"
      },
      "source": [
        "## What is JSON?\n",
        "\n",
        "**JSON (JavaScript Object Notation)** is a lightweight data-interchange format. It is easy for humans to read and write, and easy for machines to parse and generate. JSON is built on two structures:\n",
        "\n",
        "1.  A collection of name/value pairs. In various languages, this is realized as an `object`, `record`, `struct`, `dictionary`, `hash table`, `keyed list`, or `associative array`.\n",
        "2.  An ordered list of values. In most languages, this is realized as an `array`, `vector`, `list`, or `sequence`.\n",
        "\n",
        "### Key Concepts of JSON:\n",
        "\n",
        "*   **Human-Readable**: JSON is designed to be easily readable by humans.\n",
        "*   **Lightweight**: It has minimal formatting overhead, making it efficient for data transmission.\n",
        "*   **Language Independent**: Although derived from JavaScript, JSON is a language-independent data format. Parsers and generators exist for many programming languages.\n",
        "*   **Self-Describing**: JSON's structure is typically clear and easy to understand.\n",
        "*   **Hierarchical Structure**: Data is organized in a tree-like or nested structure using objects and arrays.\n",
        "\n",
        "### JSON Data Types:\n",
        "\n",
        "*   **Objects**: An unordered set of name/value pairs. An object begins with `{` (left brace) and ends with `}` (right brace). Each name is followed by a `:` (colon) and the name/value pairs are separated by `,` (comma).\n",
        "    Example: `{\"name\": \"Alice\", \"age\": 30}`\n",
        "\n",
        "*   **Arrays**: An ordered collection of values. An array begins with `[` (left bracket) and ends with `]` (right bracket). Values are separated by `,` (comma).\n",
        "    Example: `[\"apple\", \"banana\", \"cherry\"]`\n",
        "\n",
        "*   **Strings**: A sequence of zero or more Unicode characters, enclosed in double quotes. Backslash escapes are used.\n",
        "    Example: `\"Hello, World!\"`\n",
        "\n",
        "*   **Numbers**: An integer or a floating-point number.\n",
        "    Example: `123`, `3.14`, `-5`\n",
        "\n",
        "*   **Booleans**: `true` or `false`.\n",
        "\n",
        "*   **`null`**: An empty value.\n",
        "\n",
        "### Common Uses:\n",
        "\n",
        "*   **Data Exchange**: Commonly used when exchanging data between a web server and a web application.\n",
        "*   **Configuration Files**: Many applications use JSON for configuration settings.\n",
        "*   **APIs**: It is the primary data format for many RESTful APIs.\n",
        "*   **NoSQL Databases**: Databases like MongoDB use JSON-like documents to store data.\n",
        "\n",
        "JSON's simplicity and widespread support make it a ubiquitous format for modern data interchange."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "fQJG2EQYPjP-"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_json={\n",
        "    'employees':{\n",
        "    \"Emp_ID\": [101, 102, 103, 104],\n",
        "    \"Name\": [\"Rahul Sharma\", \"Priya Patel\", \"Amit Singh\", \"Neeta Rao\"],\n",
        "    \"Department\": [\"HR\", \"Finance\", \"IT\", \"Marketing\"],\n",
        "    \"Salary (₹)\": [50000, 65000, 80000, 55000],\n",
        "    \"Joining_Date\": [\"2020-01-15\", \"2019-05-22\", \"2021-11-10\", \"2022-03-05\"],\n",
        "    \"City\": [\"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "6M-P_Fe8PzBI"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data_json.json','w') as f:\n",
        "    # Use json.dump to write the data_json dictionary to the file 'f'.\n",
        "    # The 'indent=4' parameter formats the JSON output with 4-space indentation for readability.\n",
        "    json.dump(data_json,f, indent = 4)\n",
        "# json.dump() serializes python object to JSON\n",
        "# indent: improves the readability by formatting the white spaces"
      ],
      "metadata": {
        "id": "nBPxJIM4QAaK"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the JSON file"
      ],
      "metadata": {
        "id": "3qXIc5l8RMwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data_json.json','r') as file:\n",
        "  data_read = json.load(file)\n",
        "# Printing the data from json to a more readable format\n",
        "print('Employee Details:')\n",
        "# Determine the number of employees by checking the length of any of the lists (e.g., 'Emp_ID')\n",
        "num_employees = len(data_read['employees']['Emp_ID'])\n",
        "\n",
        "# Iterate through the indices to print each employee's details\n",
        "for i in range(num_employees):\n",
        "  print(f\"Employee ID: {data_read['employees']['Emp_ID'][i]}\")\n",
        "  print(f\"Name: {data_read['employees']['Name'][i]}\")\n",
        "  print(f\"Joining Date: {data_read['employees']['Joining_Date'][i]}\")\n",
        "  print(f\"Department: {data_read['employees']['Department'][i]}\")\n",
        "  print(\"--------------------\") # Separator for better readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceP-94BeQbAR",
        "outputId": "c871bd24-f61a-48ca-ef8f-fd47f5554079"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee Details:\n",
            "Employee ID: 101\n",
            "Name: Rahul Sharma\n",
            "Joining Date: 2020-01-15\n",
            "Department: HR\n",
            "--------------------\n",
            "Employee ID: 102\n",
            "Name: Priya Patel\n",
            "Joining Date: 2019-05-22\n",
            "Department: Finance\n",
            "--------------------\n",
            "Employee ID: 103\n",
            "Name: Amit Singh\n",
            "Joining Date: 2021-11-10\n",
            "Department: IT\n",
            "--------------------\n",
            "Employee ID: 104\n",
            "Name: Neeta Rao\n",
            "Joining Date: 2022-03-05\n",
            "Department: Marketing\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the indices to print each employee's details\n",
        "for i in range(num_employees):\n",
        "  print(f\"Employee ID: {data_read['employees']['Emp_ID'][i]} Name: {data_read['employees']['Name'][i]} Joining Date: {data_read['employees']['Joining_Date'][i]} Department: {data_read['employees']['Department'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcMCnDDXTWLN",
        "outputId": "9b357175-0f01-41ca-96fa-8f8d95200b38"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee ID: 101 Name: Rahul Sharma Joining Date: 2020-01-15 Department: HR\n",
            "Employee ID: 102 Name: Priya Patel Joining Date: 2019-05-22 Department: Finance\n",
            "Employee ID: 103 Name: Amit Singh Joining Date: 2021-11-10 Department: IT\n",
            "Employee ID: 104 Name: Neeta Rao Joining Date: 2022-03-05 Department: Marketing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVY3VE88UKTt"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV (Comma-Separated Values)\n",
        "# Tabular data with commas as delimiters\n",
        "\n",
        "| Aspect | Description |\n",
        "|--------|-------------|\n",
        "| **Structure** | Rows and columns (tabular) |\n",
        "| **Delimiter** | Comma (`,`) |\n",
        "| **Human Readable** | Yes |\n",
        "| **Machine Readable** | Easy (many parsers available) |\n",
        "| **File Size** | Compact |\n",
        "| **Best For** | Spreadsheets, database exports, simple datasets |\n",
        "| **Limitations** | Issues with commas in data, no data types |\n",
        "| **Example** |\n",
        "```\n",
        "                Name    Age    City\n",
        "                John    30     NY\n",
        "                Meera   25     Mumbai\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# TSV (Tab-Separated Values)\n",
        "# Tabular data with tabs as delimiters\n",
        "\n",
        "| Aspect | Description |\n",
        "|--------|-------------|\n",
        "| **Structure** | Rows and columns (tabular) |\n",
        "| **Delimiter** | Tab (`\\t`) |\n",
        "| **Human Readable** | Yes (aligned columns) |\n",
        "| **Machine Readable** | Easy |\n",
        "| **File Size** | Compact |\n",
        "| **Best For** | Data with commas, bioinformatics, legacy systems |\n",
        "| **Advantage** | Avoids comma conflicts |\n",
        "| **Example** | ```\n",
        "                Name    Age    City\n",
        "                John    30     NY\n",
        "                Meera   25     Mumbai\n",
        "``` |\n",
        "\n",
        "---\n",
        "\n",
        "# JSON (JavaScript Object Notation)\n",
        "# Hierarchical data with key-value pairs\n",
        "\n",
        "| Aspect | Description |\n",
        "|--------|-------------|\n",
        "| **Structure** | Nested objects/arrays (hierarchical) |\n",
        "| **Delimiter** | Braces `{}`, brackets `[]`, colons `:` |\n",
        "| **Human Readable** | Yes (with proper formatting) |\n",
        "| **Machine Readable** | Excellent (native for web) |\n",
        "| **File Size** | Larger (verbose with keys) |\n",
        "| **Best For** | APIs, web applications, complex/nested data |\n",
        "| **Advantages** | Supports data types, nesting, self-describing |\n",
        "| **Example** | ```\n",
        "json\n",
        "{\n",
        "  \"employees\": [\n",
        "    {\"name\": \"John\", \"age\": 30, \"city\": \"NY\"},\n",
        "    {\"name\": \"Meera\", \"age\": 25, \"city\": \"Mumbai\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Quick Comparison Table\n",
        "\n",
        "| Feature | TXT | CSV | TSV | JSON |\n",
        "|---------|-----|-----|-----|------|\n",
        "| **Data Structure** | None | Tabular | Tabular | Hierarchical |\n",
        "| **Metadata Support** | No | No | No | Yes |\n",
        "| **Data Types** | No | No | No | Yes (string, number, boolean, null, array, object) |\n",
        "| **Nested Data** | No | No | No | Yes |\n",
        "| **Parsing Speed** | N/A | Fast | Fast | Medium |\n",
        "| **File Size** | Varies | Small | Small | Medium-Large |\n",
        "| **Standardization** | None | RFC 4180 | IANA | ECMA-404 |\n",
        "| **Common Use** | Notes, logs | Excel, databases | Data science | APIs, configs |\n",
        "\n",
        "---\n",
        "\n",
        "# When to Use Each\n",
        "\n",
        "- **TXT**: Simple notes, logs, configuration files\n",
        "- **CSV**: Spreadsheet data, database exports, simple datasets\n",
        "- **TSV**: Data containing commas, genetic data, R programming\n",
        "- **JSON**: APIs, web apps, configuration, complex/nested data"
      ],
      "metadata": {
        "id": "GQV9oGTzU7Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________________________\n",
        "\n",
        "-------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "_____________________________________________________________"
      ],
      "metadata": {
        "id": "NihwN-zcnIBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing: Scaling, Encoding, Normalization with Sci-Kit-Learn in Python.\n",
        "\n",
        "# Data Preprocessing: The Foundation of Machine Learning\n",
        "\n",
        "Data preprocessing is the crucial process of cleaning, transforming, and organizing raw data into a structured format that machine learning algorithms can interpret. It is necessary because raw data is often incomplete, noisy, and inconsistent, which can lead to biased, inaccurate, or inefficient models.\n",
        "\n",
        "## Why Data Preprocessing is Necessary\n",
        "\n",
        "- **Improved Accuracy**: It removes errors and handles outliers, ensuring the model learns from high-quality, reliable information.\n",
        "- **Handling Missing Values**: It allows filling, imputing, or removing missing data, preventing model errors during training.\n",
        "- **Consistency**: It standardizes data from different sources into a uniform format.\n",
        "- **Efficiency and Performance**: Techniques like scaling and normalization ensure that features with larger magnitudes do not dominate others, enhancing model convergence and accuracy.\n",
        "- **Dimensionality Reduction**: It removes irrelevant or duplicate information, reducing computational complexity.\n",
        "\n",
        "## Key Data Preprocessing Techniques\n",
        "\n",
        "- **Data Cleaning**: Handling missing values, removing duplicates, and managing outliers.\n",
        "- **Data Integration**: Merging data from multiple sources.\n",
        "- **Data Transformation**: Normalization, scaling, and encoding categorical data into numerical formats.\n",
        "- **Data Reduction**: Reducing the volume of data while keeping its integrity.\n",
        "\n",
        "## The 7-Step Data Preprocessing Process\n",
        "\n",
        "Based on industry best practices, data preprocessing typically follows these steps:\n",
        "\n",
        "### Step 1: Data Collection\n",
        "Gathering raw data from various sources such as databases, APIs, sensors, surveys, and files. The quality of data at this stage directly impacts all subsequent steps.\n",
        "\n",
        "### Step 2: Data Cleaning\n",
        "- **Handling Missing Values**: Using techniques like mean/median/mode imputation, interpolation, or removing rows/columns with excessive missing data.\n",
        "- **Dealing with Noisy Data**: Managing outliers and errors through binning, regression, or clustering.\n",
        "- **Eliminating Duplicates**: Identifying and removing redundant records.\n",
        "\n",
        "### Step 3: Data Integration\n",
        "Combining data from multiple sources into a unified dataset. This involves handling schema integration, addressing redundancy, and ensuring consistency across merged datasets.\n",
        "\n",
        "### Step 4: Data Transformation\n",
        "- **Normalization**: Scaling data to a standard range (e.g., 0-1 using Min-Max scaling)\n",
        "- **Standardization**: Centering data around mean=0 with unit variance (Z-score scaling)\n",
        "- **Aggregation**: Summarizing data to higher levels for analysis\n",
        "- **Feature Engineering**: Creating new features from existing data to enhance model performance\n",
        "\n",
        "### Step 5: Data Reduction\n",
        "Reducing dataset size while preserving important information through techniques like Principal Component Analysis (PCA) and feature selection.\n",
        "\n",
        "### Step 6: Encoding Categorical Variables\n",
        "Converting categorical data into numerical formats using techniques like:\n",
        "- **Label Encoding**: Assigning numerical labels to categories\n",
        "- **One-Hot Encoding**: Creating binary columns for each category\n",
        "\n",
        "### Step 7: Splitting the Dataset\n",
        "Dividing data into training, validation, and test sets to properly evaluate model performance and generalization.\n",
        "\n",
        "## Common Interview Questions\n",
        "\n",
        "1. **What is the difference between normalization and standardization?**\n",
        "   Normalization scales data between 0 and 1, while standardization centers data around a mean of 0 with a standard deviation of 1.\n",
        "\n",
        "2. **How do you handle missing data?**\n",
        "   Methods include deletion, mean/median/mode imputation, or using algorithms that support missing values.\n",
        "\n",
        "3. **Why do we encode categorical data?**\n",
        "   Machine learning models require numerical input; encoding turns labels into numeric values like One-Hot Encoding or Label Encoding.\n",
        "\n",
        "4. **What is feature scaling and why is it needed?**\n",
        "   Scaling adjusts numerical features to a similar range, crucial for algorithms that calculate distances, like KNN or SVM.\n",
        "\n",
        "5. **What is the impact of not preprocessing data?**\n",
        "   Garbage In, Garbage Out: The model will produce inaccurate, biased, or inconsistent predictions.\n",
        "\n",
        "## Popular Tools and Libraries for Data Preprocessing\n",
        "\n",
        "- **Pandas**: Powerful library for data manipulation and analysis with DataFrame structures\n",
        "- **NumPy**: Foundation for scientific computing with high-performance arrays\n",
        "- **Scikit-learn**: Comprehensive suite of preprocessing tools including scalers, encoders, and dimensionality reduction\n",
        "- **TensorFlow Data Validation (TFDV)**: For data exploration, validation, and analysis\n",
        "- **Apache Spark**: Distributed computing framework for large-scale data processing\n",
        "- **OpenRefine**: Open-source tool for data cleaning and transformation\n",
        "- **Dask**: Parallel computing library for handling large datasets efficiently"
      ],
      "metadata": {
        "id": "nMGu__nGlP4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Differences: Data Wrangling vs Data Preprocessing\n",
        "\n",
        "| Aspect | Data Wrangling | Data Preprocessing |\n",
        "|--------|---------------|-------------------|\n",
        "| **Scope** | A broad process including cleaning, structuring, enriching, and validating data from various sources. | A focused set of manipulation or dropping of data strictly to ensure or enhance the performance of a specific machine learning model. |\n",
        "| **Timing** | Occurs iteratively and interactively during the exploratory analysis and model-building phases. | Typically performed once at the beginning of the data pipeline, right after data ingestion and before the iterative analysis begins. |\n",
        "| **Flexibility** | More flexible and exploratory, adapting to data changes and analyst needs in an ad-hoc manner. | Tends to be a predefined, more automatic process, often scripted, once the data format is understood. |\n",
        "| **Goal** | To transform raw, potentially messy data into a clean, structured, and usable format for better decision-making and analysis in general. | To prepare the data to fit the technical requirements of a specific machine learning algorithm (e.g., handling missing values, encoding categorical variables, feature scaling). |\n",
        "| **Tools** | Often uses self-service, visual tools like Trifacta Wrangler or general-purpose languages like Python (Pandas) and R. | Often integrated into larger data processing frameworks (like Apache Spark for big data) or data science libraries that can handle automated, repeatable scripts. |"
      ],
      "metadata": {
        "id": "Q-lGCuFhn9wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interview Answer: Determining Feature Importance in Machine Learning\n",
        "\n",
        "## How to Determine Which Feature a Model Will Consider More Important\n",
        "\n",
        "The importance of a feature (column) in a machine learning model depends on several factors:\n",
        "\n",
        "### 1. Correlation with Target Variable\n",
        "Features that have a strong statistical relationship with the target variable (what we're trying to predict) will generally be more important.\n",
        "\n",
        "**Example**: If predicting house prices, 'square footage' typically has higher correlation than 'zip code' prefix.\n",
        "\n",
        "### 2. Variance and Information Content\n",
        "Features with higher variance often contain more information. A column where all values are identical (zero variance) provides no predictive value.\n",
        "\n",
        "### 3. Feature-Target Relationship Type\n",
        "- **Linear relationships**: Linear models (regression, SVM) favor features with strong linear correlation\n",
        "- **Non-linear relationships**: Tree-based models (Random Forest, XGBoost) can capture complex non-linear patterns\n",
        "\n",
        "### 4. Scale and Units\n",
        "Features with larger numerical ranges don't necessarily have higher importance, especially after proper scaling.\n",
        "\n",
        "## How Models Actually Calculate Feature Importance\n",
        "\n",
        "### Tree-Based Models (Random Forest, XGBoost)\n",
        "These models provide built-in feature importance based on:\n",
        "- **Gini importance**: How much each feature reduces impurity when used for splitting\n",
        "- **Permutation importance**: How much model performance drops when a feature's values are randomly shuffled\n",
        "\n",
        "### Linear Models (Regression, Logistic Regression)\n",
        "Importance is indicated by:\n",
        "- **Coefficient magnitude**: Larger absolute coefficients suggest stronger impact (requires scaled features)\n",
        "- **Statistical significance**: p-values indicate confidence in the relationship\n",
        "\n",
        "### Neural Networks\n",
        "Importance can be assessed through:\n",
        "- **Gradient-based methods**: How much the output changes with input variations\n",
        "- **SHAP values**: Game-theoretic approach to explain individual predictions\n",
        "\n",
        "## Common Pitfalls in Interpreting Feature Importance\n",
        "\n",
        "❌ **Correlation ≠ Causation**: A feature may be important for prediction but not causal\n",
        "❌ **Multicollinearity**: Correlated features can split importance, making both appear less important\n",
        "❌ **Scale sensitivity**: Raw coefficients can't be compared without proper scaling\n",
        "❌ **Context matters**: Important features for one model may not transfer to another\n",
        "\n",
        "## Example Answer Structure\n",
        "\n",
        "If asked about specific columns in an interview:\n",
        "\n",
        "\"I would evaluate feature importance through multiple lenses:\n",
        "\n",
        "1. **Exploratory Analysis First**: Calculate correlation with target, check variance, and visualize relationships.\n",
        "\n",
        "2. **Model-Based Assessment**: Train a Random Forest to get Gini importance and compare with permutation importance for robustness.\n",
        "\n",
        "3. **Domain Knowledge Validation**: The most statistically important feature should make business sense. For instance, in our dataframe, 'years_of_experience' should logically be more important than 'employee_id' for predicting salary.\n",
        "\n",
        "4. **Stability Check**: Use cross-validation to ensure importance rankings are consistent across different data subsets.\n",
        "\n",
        "The 'most important' feature is ultimately the one that provides the most predictive power while maintaining interpretability and stability across validation methods.\"\n",
        "\n",
        "## Quick Checklist for Interview Response\n",
        "\n",
        "□ Identify the likely target variable first.\\\n",
        "□ Check for obvious useless features (IDs, timestamps, constants).\\\n",
        "□ Consider feature types (categorical vs numerical).\\\n",
        "□ Think about expected relationships based on domain knowledge.\\\n",
        "□ Mention specific techniques appropriate for the model type.\\\n",
        "□ Always validate with multiple methods."
      ],
      "metadata": {
        "id": "SFa4f5sIoXsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Example dataset\n",
        "numeric_data = pd.DataFrame({\n",
        "    'Salary': [30000, 45000, 60000, 80000, 120000],\n",
        "    'Age': [25, 32, 47, 51, 62]\n",
        "})\n",
        "\n",
        "# Display the dataframe\n",
        "print(\"Original Data:\")\n",
        "print(numeric_data)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "bsIgcKKyVReC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38572ec-719a-4597-c7e2-2efa5acb3f2b"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Salary  Age\n",
            "0   30000   25\n",
            "1   45000   32\n",
            "2   60000   47\n",
            "3   80000   51\n",
            "4  120000   62\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler. This scaler removes the mean and scales to unit variance.\n",
        "std_scaler = StandardScaler()\n",
        "# Fit the scaler to the numeric data and transform it.\n",
        "# fit_transform calculates the mean and standard deviation and then applies the scaling.\n",
        "scaled_std=std_scaler.fit_transform(numeric_data)\n",
        "# Create a new DataFrame from the scaled data, preserving the original column names.\n",
        "df=pd.DataFrame(scaled_std,columns=numeric_data.columns)\n",
        "\n",
        "# Display the first few rows of the scaled DataFrame.\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "iNuLMi0XpiP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac768f8-6d0f-4af1-a159-3fd2b05550f8"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Salary       Age\n",
            "0 -1.184341 -1.382872\n",
            "1 -0.704203 -0.856780\n",
            "2 -0.224065  0.270562\n",
            "3  0.416120  0.571186\n",
            "4  1.696489  1.397904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "# The .fit() method calculates the minimum and maximum values for each feature\n",
        "# in the 'numeric_data' dataset. This information is then used by .transform()\n",
        "# to scale the data to a specified range (defaulting to 0 to 1).\n",
        "# fit_transform combines both steps: fitting the scaler and then transforming the data.\n",
        "mm_scaled=min_max_scaler.fit_transform(numeric_data)\n",
        "df_1=pd.DataFrame(mm_scaled,columns=numeric_data.columns)\n",
        "print(df_1.head())\n",
        "print(df_1.mean(),'\\n',df_1.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDBgxgtBzjB2",
        "outputId": "449ac038-6cf1-4add-9c70-6b09bcb4dae7"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Salary       Age\n",
            "0  0.000000  0.000000\n",
            "1  0.166667  0.189189\n",
            "2  0.333333  0.594595\n",
            "3  0.555556  0.702703\n",
            "4  1.000000  1.000000\n",
            "Salary    0.411111\n",
            "Age       0.497297\n",
            "dtype: float64 \n",
            " Salary    0.388094\n",
            "Age       0.402058\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Encoding Categorical Data in Python\n",
        "\n",
        "Encoding categorical data is the process of converting non-numeric, text-based features into a numerical format that machine learning algorithms can understand and process, as most models require numerical input. Key Python libraries for this include scikit-learn and pandas.\n",
        "\n",
        "## Types of Categorical Data\n",
        "\n",
        "Categorical variables generally fall into two types:\n",
        "\n",
        "- **Nominal Data**: Categories without an inherent order or ranking (e.g., colors like \"red\", \"green\", \"blue\", or car brands).\n",
        "- **Ordinal Data**: Categories with a meaningful order or ranking (e.g., shirt sizes \"small\", \"medium\", \"large\", or education levels).\n",
        "\n",
        "## Common Encoding Techniques and Python Libraries\n",
        "\n",
        "The choice of encoding method depends heavily on the data type and the specific requirements of the machine learning task.\n",
        "\n",
        "### 1. One-Hot Encoding\n",
        "\n",
        "- **Description**: Creates new binary columns for each category, where a `1` indicates the presence of that category and `0` indicates its absence. This prevents the model from assuming an artificial order between categories.\n",
        "- **Use Case**: Ideal for nominal data with low cardinality (a small number of unique categories).\n",
        "- **Python Libraries**:\n",
        "    - `OneHotEncoder` from `sklearn.preprocessing`.\n",
        "    - `pd.get_dummies()` function from `pandas`.\n",
        "\n",
        "### 2. Ordinal Encoding\n",
        "\n",
        "- **Description**: Replaces each category with an integer value, preserving the intrinsic order. The order can be specified manually to ensure correctness.\n",
        "- **Use Case**: Best for ordinal data where the ranking information is important for the model.\n",
        "- **Python Libraries**:\n",
        "    - `OrdinalEncoder` from `sklearn.preprocessing`.\n",
        "\n",
        "### 3. Label Encoding\n",
        "\n",
        "- **Description**: Assigns a unique integer to each category, typically in alphabetical order. It is similar to ordinal encoding but does not explicitly account for an inherent order, which can be misleading for nominal data.\n",
        "- **Use Case**: Often used for the target variable in classification problems or with tree-based models that can handle integer inputs without assuming a numerical relationship.\n",
        "- **Python Libraries**:\n",
        "    - `LabelEncoder` from `sklearn.preprocessing`.\n",
        "\n",
        "### 4. Other Techniques for High Cardinality\n",
        "\n",
        "For variables with a large number of unique categories, other methods can be more efficient to avoid the \"curse of dimensionality\".\n",
        "- **Target Encoding**: Replaces a category with the mean of the target variable for that category. Available in the `category_encoders` library.\n",
        "- **Binary Encoding**: Converts categories to binary code, then splits the digits into separate columns, reducing dimensionality compared to one-hot encoding.\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "Encoders should be fit **only on the training data** and then used to transform both the training and test sets to prevent data leakage and ensure consistency. The `ColumnTransformer` in scikit-learn is useful for applying different encoding strategies to different columns within a single pipeline.\n",
        "\n",
        "## Quick Comparison Table\n",
        "\n",
        "| Parameter | Label Encoding | One-Hot Encoding | Ordinal Encoding |\n",
        "|-----------|---------------|------------------|------------------|\n",
        "| **Definition** | Converts each category into a unique integer | Converts categories into binary vectors | Assigns integers to categories based on order |\n",
        "| **Output Type** | Integer values | Binary vector (0/1) | Integer values respecting order |\n",
        "| **Suitable For** | Ordinal data | Nominal data | Ordinal data |\n",
        "| **Introduces Ordinality** | Yes (may be misleading for nominal data) | No | Yes |\n",
        "| **Dimensionality** | Low (single column) | High (number of unique categories becomes columns) | Low (single column) |\n",
        "| **Working** | Each category becomes unique integer | Each category becomes new binary column | Categories become ordered integers |\n",
        "| **Use Case** | Algorithms that can handle ordinal integers | Algorithms that cannot handle categorical data | Ordered categories (e.g., rating levels) |\n",
        "| **Speed / Efficiency** | Fast | Slower for high cardinality | Fast |\n",
        "\n",
        "## Implementation Examples\n",
        "\n",
        "### One-Hot Encoding with Pandas\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({'color': ['red', 'green', 'blue', 'red']})\n",
        "\n",
        "# One-hot encoding\n",
        "onehot_df = pd.get_dummies(df['color'], prefix='color', drop_first=True)\n",
        "print(onehot_df)\n",
        "```\n",
        "\n",
        "### One-Hot Encoding with Scikit-learn\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({'color': ['red', 'green', 'blue', 'red']})\n",
        "\n",
        "# Create and apply encoder\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded = encoder.fit_transform(df[['color']])\n",
        "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['color']))\n",
        "print(encoded_df)\n",
        "```\n",
        "\n",
        "### Label Encoding\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "df = pd.DataFrame({'size': ['small', 'medium', 'large', 'medium']})\n",
        "\n",
        "# Create and apply encoder\n",
        "encoder = LabelEncoder()\n",
        "df['size_encoded'] = encoder.fit_transform(df['size'])\n",
        "print(df[['size', 'size_encoded']])\n",
        "```\n",
        "\n",
        "### Ordinal Encoding with Specified Order\n",
        "```python\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data with specified order\n",
        "df = pd.DataFrame({'size': ['small', 'medium', 'large', 'medium']})\n",
        "size_order = [['small', 'medium', 'large']]\n",
        "\n",
        "# Create and apply encoder\n",
        "encoder = OrdinalEncoder(categories=size_order)\n",
        "df['size_encoded'] = encoder.fit_transform(df[['size']])\n",
        "print(df[['size', 'size_encoded']])\n",
        "```\n",
        "\n",
        "### Using ColumnTransformer for Mixed Types\n",
        "```python\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data with mixed types\n",
        "df = pd.DataFrame({\n",
        "    'color': ['red', 'green', 'blue', 'red'],\n",
        "    'size': ['small', 'medium', 'large', 'medium'],\n",
        "    'price': [100, 200, 150, 120]\n",
        "})\n",
        "\n",
        "# Define preprocessing for different column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('nominal', OneHotEncoder(drop='first'), ['color']),\n",
        "        ('ordinal', OrdinalEncoder(categories=[['small', 'medium', 'large']]), ['size'])\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns (like 'price') as-is\n",
        ")\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(df)\n",
        "print(\"Processed shape:\", X_processed.shape)"
      ],
      "metadata": {
        "id": "m-6E3tCR1lne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinality refers to the specific position, rank, or order of an item within a sequence (e.g., 1st, 2nd, 3rd) rather than its quantity or total count (cardinality). It defines the linear, ordered relationship between items, such as understanding that 4 comes after 3 and before 5."
      ],
      "metadata": {
        "id": "PB2FTHQ52wk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "cat_data=pd.DataFrame({\n",
        "    'Department':['HR','IT','Finance','HR','IT','Finance']\n",
        "\n",
        "})\n"
      ],
      "metadata": {
        "id": "_1hXCHIZz_Pz"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder"
      ],
      "metadata": {
        "id": "OfrDyp312_Z5"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le=LabelEncoder()\n",
        "cat_data_lenc=cat_data.copy(deep=True)\n",
        "cat_data_lenc['Department_encoded']=le.fit_transform(cat_data_lenc['Department'])\n",
        "\n",
        "print(cat_dat1a,'\\n',cat_data_lenc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQlDqzUj4_Lf",
        "outputId": "803661cf-3f26-4633-f793-e27ebbe42083"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Department\n",
            "0           1\n",
            "1           2\n",
            "2           0\n",
            "3           1\n",
            "4           2\n",
            "5           0 \n",
            "   Department  Department_encoded\n",
            "0         HR                   1\n",
            "1         IT                   2\n",
            "2    Finance                   0\n",
            "3         HR                   1\n",
            "4         IT                   2\n",
            "5    Finance                   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use LabelEncoding only for target variable"
      ],
      "metadata": {
        "id": "YxuDO9MOR4J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one-hot encoding on the 'Department' column.\n",
        "# pd.get_dummies converts categorical variable into dummy/indicator variables.\n",
        "# 'prefix' adds a prefix to the new column names (e.g., 'Department_HR', 'Department_IT').\n",
        "# 'drop_first=True' prevents multicollinearity by dropping the first category's column.\n",
        "cat_data_npenc=cat_data.copy(deep=True)\n",
        "cat_data_npenc=pd.get_dummies(cat_data_npenc['Department'],prefix='Department',drop_first=False)\n",
        "print(cat_data_npenc)\n",
        "print('\\n', cat_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeXDISqL4_sX",
        "outputId": "0e5ad1df-56cb-492c-8c55-68b5edc2e5f7"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Department_Finance  Department_HR  Department_IT\n",
            "0               False           True          False\n",
            "1               False          False           True\n",
            "2                True          False          False\n",
            "3               False           True          False\n",
            "4               False          False           True\n",
            "5                True          False          False\n",
            "\n",
            "   Department\n",
            "0         HR\n",
            "1         IT\n",
            "2    Finance\n",
            "3         HR\n",
            "4         IT\n",
            "5    Finance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization: Two Distinct Meanings in Data\n",
        "\n",
        "The term \"normalization\" refers to two distinct processes depending on the context: **database normalization** (structuring data to eliminate redundancy) and **data (feature) normalization** (scaling numerical data for machine learning).\n",
        "\n",
        "## 1. Data Normalization in Machine Learning (Feature Scaling)\n",
        "\n",
        "In machine learning and data science, normalization (often used interchangeably with feature scaling) is the process of transforming numerical features to a similar scale. This prevents features with large ranges from dominating the model and helps algorithms, especially those using gradient descent or distance calculations (like k-NN or SVMs), converge faster and perform better.\n",
        "\n",
        "### Why is it needed?\n",
        "- **Equalizes Feature Influence**: Prevents features with larger magnitudes from dominating those with smaller values.\n",
        "- **Improves Algorithm Performance**: Many algorithms (like SVM, KNN, neural networks) assume or perform better with scaled data.\n",
        "- **Speeds Up Convergence**: Gradient descent converges faster when features are on a similar scale.\n",
        "\n",
        "### Types of Data Normalization\n",
        "\n",
        "| Technique | Description | Formula Concept | Best Use Case | Python Library |\n",
        "|-----------|-------------|-----------------|---------------|----------------|\n",
        "| **Min-Max Scaling** | Scales data to a fixed range (usually 0 to 1), preserving relative relationships. | (x - min) / (max - min) | When you need bounded data (e.g., neural networks) | `sklearn.preprocessing.MinMaxScaler` |\n",
        "| **Z-Score Normalization (Standardization)** | Centers data around a mean of 0 with a standard deviation of 1. Handles outliers better than min-max. | (x - mean) / std | Default for many ML algorithms; when data is roughly Gaussian | `sklearn.preprocessing.StandardScaler` |\n",
        "| **Robust Scaling** | Uses median and interquartile range (IQR) to scale data, making it robust against outliers. | (x - median) / (Q3 - Q1) | When your data contains significant outliers | `sklearn.preprocessing.RobustScaler` |\n",
        "| **L2 Normalization** | Scales individual samples to have a unit norm (length of 1), focusing on direction rather than magnitude. | x / ||x||₂ | Text classification, clustering where direction matters | `sklearn.preprocessing.normalize` |\n",
        "\n",
        "### Implementation Examples\n",
        "\n",
        "```python\n",
        "# Using Scikit-learn for feature scaling\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample data with an outlier\n",
        "data = pd.DataFrame({'feature': np.append(np.random.normal(100, 20, 100), [1000])})\n",
        "\n",
        "# Min-Max Scaling (to range [0, 1])\n",
        "minmax_scaler = MinMaxScaler()\n",
        "data_minmax = minmax_scaler.fit_transform(data)\n",
        "\n",
        "# Z-Score Standardization (mean=0, std=1)\n",
        "std_scaler = StandardScaler()\n",
        "data_std = std_scaler.fit_transform(data)\n",
        "\n",
        "# Robust Scaling (using median and IQR)\n",
        "robust_scaler = RobustScaler()\n",
        "data_robust = robust_scaler.fit_transform(data)\n",
        "\n",
        "print(\"Original - Mean: {:.2f}, Std: {:.2f}\".format(data.mean()[0], data.std()[0]))\n",
        "print(\"MinMax - Min: {:.2f}, Max: {:.2f}\".format(data_minmax.min(), data_minmax.max()))\n",
        "print(\"Standardized - Mean: {:.2f}, Std: {:.2f}\".format(data_std.mean(), data_std.std()))\n",
        "print(\"Robust - Median-based scaling, less affected by outlier\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# L2 Normalization (Euclidean Normalization)\n",
        "\n",
        "L2 normalization converts row vectors into \"Pure Directions\" by dividing each vector by its Euclidean norm (magnitude). This scales each sample to have a unit norm (length of 1), preserving only the direction of the vector while discarding magnitude information.\n",
        "\n",
        "## Mathematical Definition\n",
        "\n",
        "For a vector **x** = [x₁, x₂, ..., xₙ], the L2-normalized vector is:\n",
        "\n",
        "**x_normalized** = **x** / ||**x**||₂\n",
        "\n",
        "where ||**x**||₂ = √(x₁² + x₂² + ... + xₙ²) is the Euclidean norm (L2 norm).\n",
        "\n",
        "## Key Properties\n",
        "\n",
        "- **Unit Length**: After normalization, ||**x_normalized**||₂ = 1\n",
        "- **Direction Preserved**: The relative proportions between components remain the same\n",
        "- **Magnitude Removed**: Only the direction matters, not the scale\n",
        "- **Pure Directions**: Each vector becomes a point on the unit sphere\n",
        "\n",
        "## Visual Example\n",
        "\n",
        "For a 2D vector [3, 4]:\n",
        "- Euclidean norm = √(3² + 4²) = √(9 + 16) = √25 = 5\n",
        "- Normalized vector = [3/5, 4/5] = [0.6, 0.8]\n",
        "- Check: √(0.6² + 0.8²) = √(0.36 + 0.64) = √1 = 1\n",
        "\n",
        "## Implementation in Python\n",
        "\n",
        "### Manual Implementation\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def l2_normalize_manual(vector):\n",
        "    \"\"\"\n",
        "    Manually L2-normalize a vector by dividing by its Euclidean norm.\n",
        "    \"\"\"\n",
        "    norm = np.sqrt(np.sum(np.square(vector)))\n",
        "    if norm == 0:\n",
        "        return vector  # Avoid division by zero\n",
        "    return vector / norm\n",
        "\n",
        "# Example with 2D vector\n",
        "vector = np.array([3, 4])\n",
        "normalized = l2_normalize_manual(vector)\n",
        "print(f\"Original vector: {vector}\")\n",
        "print(f\"Euclidean norm: {np.sqrt(np.sum(vector**2)):.2f}\")\n",
        "print(f\"Normalized vector: {normalized}\")\n",
        "print(f\"Normalized norm: {np.sqrt(np.sum(normalized**2)):.2f}\")"
      ],
      "metadata": {
        "id": "zMiebVSOTRr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def l2_normalize_manual(vector):\n",
        "    \"\"\"\n",
        "    Manually L2-normalize a vector by dividing by its Euclidean norm.\n",
        "    \"\"\"\n",
        "    norm = np.sqrt(np.sum(np.square(vector)))\n",
        "    if norm == 0:\n",
        "        return vector  # Avoid division by zero\n",
        "    return vector / norm\n",
        "\n",
        "# Example with 2D vector\n",
        "vector = np.array([3, 4])\n",
        "normalized = l2_normalize_manual(vector)\n",
        "print(f\"Original vector: {vector}\")\n",
        "print(f\"Euclidean norm: {np.sqrt(np.sum(vector**2)):.2f}\")\n",
        "print(f\"Normalized vector: {normalized}\")\n",
        "print(f\"Normalized norm: {np.sqrt(np.sum(normalized**2)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCMkqVbzTRXe",
        "outputId": "7c69c9f1-6264-4969-a1bb-ddd05da7e9ab"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vector: [3 4]\n",
            "Euclidean norm: 5.00\n",
            "Normalized vector: [0.6 0.8]\n",
            "Normalized norm: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 Normalization (Euclidean Normalization)\n",
        "\n",
        "L2 normalization converts row vectors into \"Pure Directions\" by dividing each vector by its Euclidean norm (magnitude). This scales each sample to have a unit norm (length of 1), preserving only the direction of the vector while discarding magnitude information.\n",
        "\n",
        "## Mathematical Definition\n",
        "\n",
        "For a vector **x** = [x₁, x₂, ..., xₙ], the L2-normalized vector is:\n",
        "\n",
        "**x_normalized** = **x** / ||**x**||₂\n",
        "\n",
        "where ||**x**||₂ = √(x₁² + x₂² + ... + xₙ²) is the Euclidean norm (L2 norm).\n",
        "\n",
        "## Key Properties\n",
        "\n",
        "- **Unit Length**: After normalization, ||**x_normalized**||₂ = 1\n",
        "- **Direction Preserved**: The relative proportions between components remain the same\n",
        "- **Magnitude Removed**: Only the direction matters, not the scale\n",
        "- **Pure Directions**: Each vector becomes a point on the unit sphere\n",
        "\n",
        "## Visual Example\n",
        "\n",
        "For a 2D vector [3, 4]:\n",
        "- Euclidean norm = √(3² + 4²) = √(9 + 16) = √25 = 5\n",
        "- Normalized vector = [3/5, 4/5] = [0.6, 0.8]\n",
        "- Check: √(0.6² + 0.8²) = √(0.36 + 0.64) = √1 = 1\n",
        "\n",
        "## Implementation in Python\n",
        "\n",
        "### Manual Implementation\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def l2_normalize_manual(vector):\n",
        "    \"\"\"\n",
        "    Manually L2-normalize a vector by dividing by its Euclidean norm.\n",
        "    \"\"\"\n",
        "    norm = np.sqrt(np.sum(np.square(vector)))\n",
        "    if norm == 0:\n",
        "        return vector  # Avoid division by zero\n",
        "    return vector / norm\n",
        "\n",
        "# Example with 2D vector\n",
        "vector = np.array([3, 4])\n",
        "normalized = l2_normalize_manual(vector)\n",
        "print(f\"Original vector: {vector}\")\n",
        "print(f\"Euclidean norm: {np.sqrt(np.sum(vector**2)):.2f}\")\n",
        "print(f\"Normalized vector: {normalized}\")\n",
        "print(f\"Normalized norm: {np.sqrt(np.sum(normalized**2)):.2f}\")\n",
        "```\n",
        "\n",
        "### Using Scikit-learn\n",
        "```python\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "# Single vector (reshape to 2D array: samples × features)\n",
        "vector = np.array([3, 4]).reshape(1, -1)\n",
        "normalized = normalize(vector, norm='l2')\n",
        "print(f\"Scikit-learn normalized: {normalized[0]}\")\n",
        "\n",
        "# Multiple vectors at once\n",
        "X = np.array([[3, 4], [1, 1], [0, 5], [2, 2]])\n",
        "X_normalized = normalize(X, norm='l2')\n",
        "print(\"\\nOriginal vectors:\\n\", X)\n",
        "print(\"\\nL2-normalized vectors (unit length):\\n\", X_normalized)\n",
        "\n",
        "# Verify each row has unit length\n",
        "norms = np.sqrt(np.sum(X_normalized**2, axis=1))\n",
        "print(\"\\nNorms after normalization:\", norms)\n",
        "```\n",
        "\n",
        "### Using NumPy (Vectorized)\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Multiple vectors\n",
        "X = np.array([[3, 4], [1, 1], [0, 5], [2, 2]])\n",
        "\n",
        "# Calculate norms for each row (keepdims=True for broadcasting)\n",
        "norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "# Avoid division by zero\n",
        "norms[norms == 0] = 1\n",
        "X_normalized = X / norms\n",
        "\n",
        "print(\"Vectorized NumPy normalization:\\n\", X_normalized)\n",
        "```\n",
        "\n",
        "## When to Use L2 Normalization\n",
        "\n",
        "### Ideal Use Cases:\n",
        "- **Text Classification**: Document-term matrices where document length varies\n",
        "- **Clustering**: When you care about direction (e.g., cosine similarity)\n",
        "- **Recommendation Systems**: User/item profiles where magnitude isn't meaningful\n",
        "- **Neural Networks**: Input preprocessing for models sensitive to scale\n",
        "- **Cosine Similarity**: Pre-normalized vectors make cosine similarity just the dot product\n",
        "\n",
        "### Example: Document Similarity\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Term frequency vectors for 3 documents\n",
        "# Rows: documents, Columns: word frequencies\n",
        "doc_vectors = np.array([\n",
        "    [5, 2, 0, 1, 3],  # Document A\n",
        "    [10, 4, 0, 2, 6], # Document B (exactly 2x Document A)\n",
        "    [0, 1, 8, 0, 1]   # Document C (different topic)\n",
        "])\n",
        "\n",
        "print(\"Original document vectors:\")\n",
        "print(doc_vectors)\n",
        "\n",
        "# L2 normalize to focus on direction, not magnitude\n",
        "doc_vectors_norm = normalize(doc_vectors, norm='l2')\n",
        "\n",
        "print(\"\\nL2-normalized vectors (unit length):\")\n",
        "print(doc_vectors_norm)\n",
        "\n",
        "# Cosine similarity (now just dot product due to normalization)\n",
        "similarity = np.dot(doc_vectors_norm, doc_vectors_norm.T)\n",
        "print(\"\\nCosine similarity matrix:\")\n",
        "print(similarity)\n",
        "\n",
        "# Note: Documents A and B have perfect similarity (1.0) despite different lengths\n",
        "# because they have the same word proportions/direction\n",
        "```\n",
        "\n",
        "## Comparison with Other Normalization Methods\n",
        "\n",
        "| Method | Formula | Output Range | When to Use |\n",
        "|--------|---------|--------------|-------------|\n",
        "| **L2 Normalization** | x / √(∑xᵢ²) | Unit sphere | Direction matters, magnitude irrelevant |\n",
        "| **L1 Normalization** | x / ∑|xᵢ| | Sum = 1 | Sparse data, probabilities |\n",
        "| **Min-Max Scaling** | (x - min)/(max - min) | [0, 1] | Bounded data needed |\n",
        "| **Standardization** | (x - μ)/σ | Mean=0, std=1 | Normal distribution assumption |\n",
        "\n",
        "## Important Considerations\n",
        "\n",
        "⚠️ **Division by Zero**: Handle vectors with zero norm (all zeros) to avoid errors.\\\n",
        "⚠️ **Sparse Data**: For sparse matrices, use `normalize` with `axis=1` and `copy=False` for efficiency.\\\n",
        "⚠️ **Information Loss**: Discards magnitude information—use only when magnitude isn't meaningful.\\\n",
        "⚠️ **Outliers**: Less affected by outliers than min-max scaling, but extreme values still influence the norm\n",
        "\n",
        "## Practical Example: Image Processing\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Simulate image patches as vectors (e.g., 8×8 patches = 64 features)\n",
        "np.random.seed(42)\n",
        "n_patches = 100\n",
        "patch_vectors = np.random.randn(n_patches, 64)\n",
        "\n",
        "# L2 normalize each patch independently\n",
        "patches_normalized = normalize(patch_vectors, norm='l2')\n",
        "\n",
        "# Verify each patch has unit norm\n",
        "norms = np.linalg.norm(patches_normalized, axis=1)\n",
        "print(f\"Mean norm after normalization: {norms.mean():.6f}\")\n",
        "print(f\"All norms = 1? {np.allclose(norms, 1.0)}\")\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "L2 normalization converts vectors to \"pure directions\" by:\n",
        "1. Computing the Euclidean norm (√(x₁² + x₂² + ...))\n",
        "2. Dividing each component by this norm\n",
        "3. Resulting in unit-length vectors (norm = 1)\n",
        "\n",
        "This is ideal when you care about the relative proportions of features (direction) rather than their absolute magnitudes.\n",
        "```"
      ],
      "metadata": {
        "id": "JulvY8clWGia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When to Use Different Normalization Techniques\n",
        "\n",
        "## Quick Decision Guide\n",
        "\n",
        "| Use This Technique | When... | Example Scenarios |\n",
        "|-------------------|---------|-------------------|\n",
        "| **L2 Normalization** | Direction matters more than magnitude; you want unit vectors | Text documents, user preferences, gene expression profiles, cosine similarity |\n",
        "| **L1 Normalization** | You need sparse outputs or probability distributions | Term frequencies, feature selection, LASSO regression |\n",
        "| **Min-Max Scaling** | You need bounded data (e.g., [0,1] or [-1,1]); neural networks | Image pixels (0-255), neural network inputs, distance-based algorithms |\n",
        "| **Standardization (Z-score)** | Your data is roughly Gaussian; you need to handle outliers; default choice | Most ML algorithms (linear regression, SVM, PCA), normally distributed features |\n",
        "| **Robust Scaling** | Your data has significant outliers; median/IQR are better than mean/std | Income data, sensor readings with anomalies, financial data |\n",
        "\n",
        "## Detailed Decision Tree\n",
        "\n",
        "### Use L2 Normalization WHEN:\n",
        "\n",
        "✅ **Direction matters more than magnitude**\n",
        "- Document similarity: \"Which documents have similar topics regardless of length?\"\n",
        "- User profiling: \"Which users have similar interests regardless of activity level?\"\n",
        "\n",
        "✅ **You're using cosine similarity**\n",
        "- Recommendation systems\n",
        "- Information retrieval\n",
        "- Clustering high-dimensional data\n",
        "\n",
        "✅ **Text/NLP applications**\n",
        "- TF-IDF vectors\n",
        "- Word embeddings\n",
        "- Document classification\n",
        "\n",
        "✅ **Magnitude is arbitrary or misleading**\n",
        "- Gene expression: \"Which genes have similar expression patterns regardless of absolute levels?\"\n",
        "- Sensor data: \"Which sensors show similar patterns regardless of calibration?\"\n",
        "\n",
        "### Use L1 Normalization WHEN:\n",
        "\n",
        "✅ **You need probability distributions** (sum = 1)\n",
        "- Topic models\n",
        "- Markov chains\n",
        "- Probability mass functions\n",
        "\n",
        "✅ **Sparsity is desired**\n",
        "- Feature selection\n",
        "- LASSO regression\n",
        "- Sparse coding\n",
        "\n",
        "✅ **Outliers should have less impact** (L1 is more robust than L2)\n",
        "- Robust statistics\n",
        "- Anomaly detection baselines\n",
        "\n",
        "### Use Min-Max Scaling WHEN:\n",
        "\n",
        "✅ **You have bounded data requirements**\n",
        "- Neural networks with sigmoid/tanh activation (expect [0,1] or [-1,1])\n",
        "- Image processing (pixel values 0-255 → [0,1])\n",
        "- Computer vision tasks\n",
        "\n",
        "✅ **You need to preserve zero values**\n",
        "- Sparse data where zeros should stay zeros\n",
        "- Binary/indicator variables\n",
        "\n",
        "✅ **Algorithm expects specific input range**\n",
        "- Distance-based algorithms (KNN, K-means) with non-Euclidean distances\n",
        "- Gradient descent with specific learning rates\n",
        "\n",
        "### Use Standardization (Z-score) WHEN:\n",
        "\n",
        "✅ **You're unsure what to use (default choice!)**\n",
        "- Most machine learning algorithms\n",
        "- Linear regression, logistic regression\n",
        "- SVM, neural networks (with proper activation)\n",
        "\n",
        "✅ **Your data is approximately normally distributed**\n",
        "- Height, weight, test scores\n",
        "- Natural phenomena\n",
        "\n",
        "✅ **You need to handle outliers gracefully**\n",
        "- Less sensitive to outliers than min-max\n",
        "- Preserves outlier information without squashing inliers\n",
        "\n",
        "✅ **Algorithm assumes centered data**\n",
        "- PCA (Principal Component Analysis)\n",
        "- Linear discriminant analysis\n",
        "- Regularized regression\n",
        "\n",
        "### Use Robust Scaling WHEN:\n",
        "\n",
        "✅ **Your data has significant outliers**\n",
        "- Income distributions (billionaires skew data)\n",
        "- Sensor networks with faulty readings\n",
        "- Network traffic with spikes\n",
        "\n",
        "✅ **Median and IQR are more meaningful than mean/std**\n",
        "- Skewed distributions\n",
        "- Non-Gaussian data\n",
        "- Real-world messy data\n",
        "\n",
        "✅ **You want outlier-resistant preprocessing**\n",
        "- Fraud detection\n",
        "- Anomaly detection training\n",
        "- Industrial process monitoring\n",
        "\n",
        "## Algorithm-Specific Recommendations\n",
        "\n",
        "| Algorithm | Recommended Scaling | Why? |\n",
        "|-----------|--------------------|------|\n",
        "| **Linear Regression** | Standardization | Assumes normally distributed features |\n",
        "| **Logistic Regression** | Standardization | Gradient descent converges faster |\n",
        "| **SVM (RBF kernel)** | Standardization | Distance-based; sensitive to feature scales |\n",
        "| **K-Nearest Neighbors** | Standardization or Min-Max | Distance-based; all features should contribute equally |\n",
        "| **K-Means Clustering** | Standardization | Euclidean distance; prevents one feature dominating |\n",
        "| **Neural Networks** | Min-Max or Standardization | Activation function ranges; gradient stability |\n",
        "| **PCA** | Standardization | Variance-based; prevents high-variance features dominating |\n",
        "| **Decision Trees/Random Forest** | **None needed!** | Tree-based models are scale-invariant |\n",
        "| **Naive Bayes** | None typically needed | Handles different scales naturally |\n",
        "| **L1/L2 Regularization** | Standardization | Regularization penalizes coefficients equally |\n",
        "\n",
        "## Practical Examples\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.datasets import make_blobs, load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example 1: L2 Normalization for Document Similarity\n",
        "print(\"=\"*60)\n",
        "print(\"SCENARIO 1: Document Similarity (Use L2 Normalization)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Documents with different lengths but similar topics\n",
        "documents = np.array([\n",
        "    [5, 2, 0, 1, 3],   # Short document about tech\n",
        "    [10, 4, 0, 2, 6],  # Long document about same tech topics\n",
        "    [0, 1, 8, 0, 1],   # Document about different topic (art)\n",
        "])\n",
        "\n",
        "# Without normalization: document 1 and 2 appear very different due to length\n",
        "print(\"Without normalization (raw counts):\")\n",
        "print(documents)\n",
        "print(\"Euclidean distance between doc1 and doc2:\",\n",
        "      np.linalg.norm(documents[0] - documents[1]))\n",
        "print(\"Euclidean distance between doc1 and doc3:\",\n",
        "      np.linalg.norm(documents[0] - documents[2]))\n",
        "\n",
        "# With L2 normalization: documents 1 and 2 become identical (same topic direction)\n",
        "documents_norm = normalize(documents, norm='l2')\n",
        "print(\"\\nWith L2 normalization (unit vectors):\")\n",
        "print(documents_norm)\n",
        "print(\"Euclidean distance between doc1 and doc2 (should be ~0):\",\n",
        "      np.linalg.norm(documents_norm[0] - documents_norm[1]))\n",
        "print(\"Euclidean distance between doc1 and doc3:\",\n",
        "      np.linalg.norm(documents_norm[0] - documents_norm[2]))\n",
        "\n",
        "# Example 2: Outlier Sensitivity Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 2: Data with Outliers (Use Robust Scaling)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Generate data with an outlier\n",
        "np.random.seed(42)\n",
        "normal_data = np.random.normal(100, 20, 100)\n",
        "data_with_outlier = np.append(normal_data, [1000]).reshape(-1, 1)\n",
        "\n",
        "df = pd.DataFrame({'original': data_with_outlier.flatten()})\n",
        "\n",
        "# Apply different scalers\n",
        "df['minmax'] = MinMaxScaler().fit_transform(data_with_outlier)\n",
        "df['standard'] = StandardScaler().fit_transform(data_with_outlier)\n",
        "df['robust'] = RobustScaler().fit_transform(data_with_outlier)\n",
        "\n",
        "print(\"Effect of outlier on scaling methods:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Example 3: PCA Requires Standardization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 3: PCA Analysis (Use Standardization)\")\n",
        "print(\"=\"*60)\n",
        "print(\"PCA finds directions of maximum variance.\")\n",
        "print(\"Without standardization, features with larger scales dominate.\")\n",
        "print(\"Example: if one feature is in [0,1] and another in [0,1000],\")\n",
        "print(\"the second feature will dominate PCA regardless of importance.\")\n",
        "\n",
        "# Example 4: Neural Networks Need Bounded Input\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 4: Neural Networks (Use Min-Max or Standardization)\")\n",
        "print(\"=\"*60)\n",
        "print(\"Neural networks with sigmoid/tanh activation expect inputs in [-1,1] or [0,1].\")\n",
        "print(\"Gradient descent converges faster when features are on similar scales.\")\n",
        "print(\"Images are naturally min-max scaled (pixels 0-255 → [0,1] after division by 255).\")\n",
        "\n",
        "# Example 5: Tree-Based Models Are Scale-Invariant\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 5: Random Forest / Decision Trees (No Scaling Needed)\")\n",
        "print(\"=\"*60)\n",
        "print(\"Tree-based models split on feature values independently.\")\n",
        "print(\"Scaling doesn't affect decision boundaries or importance rankings.\")\n",
        "print(\"You can skip normalization entirely for these algorithms!\")\n",
        "\n",
        "# Summary Decision Matrix\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY: WHEN TO USE EACH TECHNIQUE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "decision_matrix = pd.DataFrame({\n",
        "    'Technique': ['L2 Normalization', 'L1 Normalization', 'Min-Max Scaling',\n",
        "                  'Standardization', 'Robust Scaling', 'No Scaling'],\n",
        "    'Use When': [\n",
        "        'Direction matters, cosine similarity, text data',\n",
        "        'Sparsity needed, probability distributions',\n",
        "        'Bounded output needed, neural networks, image data',\n",
        "        'Default choice, normally distributed data, PCA',\n",
        "        'Significant outliers, skewed distributions',\n",
        "        'Tree-based models (Random Forest, XGBoost)'\n",
        "    ],\n",
        "    'Avoid When': [\n",
        "        'Magnitude information is important',\n",
        "        'Dense data, magnitude matters',\n",
        "        'Outliers present, unbounded algorithms',\n",
        "        'Very non-Gaussian data',\n",
        "        'You need bounded output',\n",
        "        'Distance-based algorithms (KNN, SVM)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(decision_matrix.to_string(index=False))"
      ],
      "metadata": {
        "id": "LZIj9gcYWpkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.datasets import make_blobs, load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example 1: L2 Normalization for Document Similarity\n",
        "print(\"=\"*60)\n",
        "print(\"SCENARIO 1: Document Similarity (Use L2 Normalization)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Documents with different lengths but similar topics\n",
        "documents = np.array([\n",
        "    [5, 2, 0, 1, 3],   # Short document about tech\n",
        "    [10, 4, 0, 2, 6],  # Long document about same tech topics\n",
        "    [0, 1, 8, 0, 1],   # Document about different topic (art)\n",
        "])\n",
        "print(documents.flatten())\n",
        "# Without normalization: document 1 and 2 appear very different due to length\n",
        "print(\"Without normalization (raw counts):\")\n",
        "print(documents)\n",
        "print(\"Euclidean distance between doc1 and doc2:\",\n",
        "      np.linalg.norm(documents[0] - documents[1]))\n",
        "print(\"Euclidean distance between doc1 and doc3:\",\n",
        "      np.linalg.norm(documents[0] - documents[2]))\n",
        "\n",
        "# With L2 normalization: documents 1 and 2 become identical (same topic direction)\n",
        "documents_norm = normalize(documents, norm='l2')\n",
        "print(\"\\nWith L2 normalization (unit vectors):\")\n",
        "print(documents_norm)\n",
        "print(\"Euclidean distance between doc1 and doc2 (should be ~0):\",\n",
        "      np.linalg.norm(documents_norm[0] - documents_norm[1]))\n",
        "print(\"Euclidean distance between doc1 and doc3:\",\n",
        "      np.linalg.norm(documents_norm[0] - documents_norm[2]))\n",
        "\n",
        "# Example 2: Outlier Sensitivity Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 2: Data with Outliers (Use Robust Scaling)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Generate data with an outlier\n",
        "np.random.seed(42)\n",
        "normal_data = np.random.normal(100, 20, 100)\n",
        "data_with_outlier = np.append(normal_data, [1000]).reshape(-1, 1)\n",
        "df = pd.DataFrame({'original': data_with_outlier.flatten()})\n",
        "\n",
        "# Apply different scalers\n",
        "df['minmax'] = MinMaxScaler().fit_transform(data_with_outlier)\n",
        "df['standard'] = StandardScaler().fit_transform(data_with_outlier)\n",
        "df['robust'] = RobustScaler().fit_transform(data_with_outlier)\n",
        "\n",
        "print(\"Effect of outlier on scaling methods:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Example 3: PCA Requires Standardization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 3: PCA Analysis (Use Standardization)\")\n",
        "print(\"=\"*60)\n",
        "print(\"PCA finds directions of maximum variance.\")\n",
        "print(\"Without standardization, features with larger scales dominate.\")\n",
        "print(\"Example: if one feature is in [0,1] and another in [0,1000],\")\n",
        "print(\"the second feature will dominate PCA regardless of importance.\")\n",
        "\n",
        "# Example 4: Neural Networks Need Bounded Input\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 4: Neural Networks (Use Min-Max or Standardization)\")\n",
        "print(\"=\"*60)\n",
        "print(\"Neural networks with sigmoid/tanh activation expect inputs in [-1,1] or [0,1].\")\n",
        "print(\"Gradient descent converges faster when features are on similar scales.\")\n",
        "print(\"Images are naturally min-max scaled (pixels 0-255 → [0,1] after division by 255).\")\n",
        "\n",
        "# Example 5: Tree-Based Models Are Scale-Invariant\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCENARIO 5: Random Forest / Decision Trees (No Scaling Needed)\")\n",
        "print(\"=\"*60)\n",
        "print(\"Tree-based models split on feature values independently.\")\n",
        "print(\"Scaling doesn't affect decision boundaries or importance rankings.\")\n",
        "print(\"You can skip normalization entirely for these algorithms!\")\n",
        "\n",
        "# Summary Decision Matrix\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY: WHEN TO USE EACH TECHNIQUE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "decision_matrix = pd.DataFrame({\n",
        "    'Technique': ['L2 Normalization', 'L1 Normalization', 'Min-Max Scaling',\n",
        "                  'Standardization', 'Robust Scaling', 'No Scaling'],\n",
        "    'Use When': [\n",
        "        'Direction matters, cosine similarity, text data',\n",
        "        'Sparsity needed, probability distributions',\n",
        "        'Bounded output needed, neural networks, image data',\n",
        "        'Default choice, normally distributed data, PCA',\n",
        "        'Significant outliers, skewed distributions',\n",
        "        'Tree-based models (Random Forest, XGBoost)'\n",
        "    ],\n",
        "    'Avoid When': [\n",
        "        'Magnitude information is important',\n",
        "        'Dense data, magnitude matters',\n",
        "        'Outliers present, unbounded algorithms',\n",
        "        'Very non-Gaussian data',\n",
        "        'You need bounded output',\n",
        "        'Distance-based algorithms (KNN, SVM)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(decision_matrix.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YbH3-o7UUDn",
        "outputId": "e5b3684e-dca7-4ab6-93d0-de7763164d40"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SCENARIO 1: Document Similarity (Use L2 Normalization)\n",
            "============================================================\n",
            "[ 5  2  0  1  3 10  4  0  2  6  0  1  8  0  1]\n",
            "Without normalization (raw counts):\n",
            "[[ 5  2  0  1  3]\n",
            " [10  4  0  2  6]\n",
            " [ 0  1  8  0  1]]\n",
            "Euclidean distance between doc1 and doc2: 6.244997998398398\n",
            "Euclidean distance between doc1 and doc3: 9.746794344808963\n",
            "\n",
            "With L2 normalization (unit vectors):\n",
            "[[0.80064077 0.32025631 0.         0.16012815 0.48038446]\n",
            " [0.80064077 0.32025631 0.         0.16012815 0.48038446]\n",
            " [0.         0.12309149 0.98473193 0.         0.12309149]]\n",
            "Euclidean distance between doc1 and doc2 (should be ~0): 0.0\n",
            "Euclidean distance between doc1 and doc3: 1.3427195790646829\n",
            "\n",
            "============================================================\n",
            "SCENARIO 2: Data with Outliers (Use Robust Scaling)\n",
            "============================================================\n",
            "Effect of outlier on scaling methods:\n",
            "          original      minmax      standard      robust\n",
            "count   101.000000  101.000000  1.010000e+02  101.000000\n",
            "mean    106.854524    0.062211 -1.582892e-16    0.417709\n",
            "std      91.561281    0.096138  1.004988e+00    4.171916\n",
            "min      47.605098    0.000000 -6.503288e-01   -2.281943\n",
            "25%      87.987226    0.042401 -2.070897e-01   -0.441964\n",
            "50%      97.687034    0.052585 -1.006235e-01    0.000000\n",
            "75%     109.934283    0.065445  3.380380e-02    0.558036\n",
            "max    1000.000000    1.000000  9.803271e+00   41.113165\n",
            "\n",
            "============================================================\n",
            "SCENARIO 3: PCA Analysis (Use Standardization)\n",
            "============================================================\n",
            "PCA finds directions of maximum variance.\n",
            "Without standardization, features with larger scales dominate.\n",
            "Example: if one feature is in [0,1] and another in [0,1000],\n",
            "the second feature will dominate PCA regardless of importance.\n",
            "\n",
            "============================================================\n",
            "SCENARIO 4: Neural Networks (Use Min-Max or Standardization)\n",
            "============================================================\n",
            "Neural networks with sigmoid/tanh activation expect inputs in [-1,1] or [0,1].\n",
            "Gradient descent converges faster when features are on similar scales.\n",
            "Images are naturally min-max scaled (pixels 0-255 → [0,1] after division by 255).\n",
            "\n",
            "============================================================\n",
            "SCENARIO 5: Random Forest / Decision Trees (No Scaling Needed)\n",
            "============================================================\n",
            "Tree-based models split on feature values independently.\n",
            "Scaling doesn't affect decision boundaries or importance rankings.\n",
            "You can skip normalization entirely for these algorithms!\n",
            "\n",
            "============================================================\n",
            "SUMMARY: WHEN TO USE EACH TECHNIQUE\n",
            "============================================================\n",
            "       Technique                                           Use When                             Avoid When\n",
            "L2 Normalization    Direction matters, cosine similarity, text data     Magnitude information is important\n",
            "L1 Normalization         Sparsity needed, probability distributions          Dense data, magnitude matters\n",
            " Min-Max Scaling Bounded output needed, neural networks, image data Outliers present, unbounded algorithms\n",
            " Standardization     Default choice, normally distributed data, PCA                 Very non-Gaussian data\n",
            "  Robust Scaling         Significant outliers, skewed distributions                You need bounded output\n",
            "      No Scaling         Tree-based models (Random Forest, XGBoost)   Distance-based algorithms (KNN, SVM)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZUssZRLWwRE"
      },
      "execution_count": 159,
      "outputs": []
    }
  ]
}